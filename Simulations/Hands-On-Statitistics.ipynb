{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook for Statistical Concepts: A Meta Data Science Focus\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "This Jupyter Notebook is designed to augment and enhance your understanding of statistical concepts crucial for success in data science roles, particularly within the context of companies like Meta. It covers a range of topics, from descriptive statistics and probability distributions to hypothesis testing, regression, and experimental design. Each concept is explained briefly and then demonstrated with Python code and visualizations.\n",
    "\n",
    "This notebook is intended for learning and exploration. It's crucial to remember that real-world data analysis often involves complex scenarios and requires deeper investigation beyond the basic examples presented here.\n",
    "\n",
    "**Note:** This is an introductory exploration. For a deeper understanding, it's recommended to consult statistical textbooks, online courses, and engage in hands-on data science projects. *The code and examples are for illustrative purposes and may need adaptation for specific datasets and analytical tasks.*\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1.  [Foundational Descriptive Statistics](#descriptive-statistics)\n",
    "    *   [1.1 Measures of Central Tendency (Mean, Median, Mode)](#measures-of-central-tendency)\n",
    "    *   [1.2 Measures of Dispersion (Range, Variance, Standard Deviation, IQR)](#measures-of-dispersion)\n",
    "    *   [1.3 Percentiles and Quantiles](#percentiles-and-quantiles)\n",
    "    *   [1.4 Data Distributions (Histograms, Density Plots, Skewness, Kurtosis)](#data-distributions)\n",
    "    *   [1.5 Data Transformation (Standardization, Normalization, Log Transformation)](#data-transformation)\n",
    "\n",
    "2.  [Probability Fundamentals](#probability-fundamentals)\n",
    "    *   [2.1 Probability Basics (Conditional Probability, Bayes' Theorem)](#probability-basics)\n",
    "    *   [2.2 Discrete Distributions (Bernoulli, Binomial, Poisson)](#discrete-distributions)\n",
    "    *   [2.3 Continuous Distributions (Normal, Exponential, Uniform)](#continuous-distributions)\n",
    "    *  [2.4 Central Limit Theorem](#central-limit-theorem)\n",
    "\n",
    "3.  [Hypothesis Testing & Statistical Inference](#hypothesis-testing-statistical-inference)\n",
    "     *   [3.1 Hypothesis Testing Framework (Null/Alternative Hypotheses, Type I/II Errors)](#hypothesis-testing-framework)\n",
    "     *  [3.2 P-values and Significance Levels](#p-values-and-significance-levels)\n",
    "    *    [3.3 Confidence Intervals](#confidence-intervals)\n",
    "    *    [3.4 Statistical Power and Sample Size](#statistical-power-and-sample-size)\n",
    "    *   [3.5 One-Sample Tests (t-test, z-test)](#one-sample-tests)\n",
    "    *   [3.6 Two-Sample Tests (Independent/Paired t-test, Chi-square)](#two-sample-tests)\n",
    "    *   [3.7 A/B Testing](#ab-testing)\n",
    "\n",
    "4.  [Correlation](#correlation)\n",
    "    *   [4.1 Correlation (Pearson and Spearman)](#correlation-pearson-and-spearman)\n",
    "\n",
    "5.  [Regression Analysis](#regression-analysis)\n",
    "    *   [5.1 Linear Regression (Simple and Multiple)](#linear-regression-simple-and-multiple)\n",
    "    *   [5.2 Logistic Regression](#logistic-regression)\n",
    "    *   [5.3 Regularization (L1/L2)](#regularization)\n",
    "\n",
    "6.  [Experimental Design & Causal Inference](#experimental-design-causal-inference)\n",
    "    *   [6.1 Experimental Design Principles (Randomization, Control/Treatment Groups)](#experimental-design-principles)\n",
    "    *   [6.2 A/B Testing in Social Networks (Network/Spillover Effects, Clustering)](#ab-testing-in-social-networks)\n",
    "    *   [6.3 Observational Studies vs. Experiments](#observational-studies-vs-experiments)\n",
    "\n",
    "7.  [Resampling Techniques](#resampling-techniques)\n",
    "    *   [7.1 Resampling Techniques (Bootstrapping, Cross-Validation)](#resampling-techniques-methods)\n",
    "\n",
    "8.  [Bias-Variance Tradeoff](#bias-variance-tradeoff)\n",
    "    *   [8.1 Bias-Variance Tradeoff](#bias-variance-tradeoff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Descriptive Statistics <a name=\"descriptive-statistics\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Descriptive statistics provide a concise summary of the main features of a dataset. In product analytics, these are essential for understanding user behavior, identifying trends, and detecting anomalies. Key metrics include:\n",
    "\n",
    "*   **Measures of Central Tendency (Mean, Median, Mode):** Describe the \"typical\" value in a dataset. Useful for understanding average user engagement, average purchase value, etc. The median is robust to outliers, making it useful when data is skewed.\n",
    "*   **Measures of Dispersion (Range, Variance, Standard Deviation, IQR):** Describe the spread or variability of the data. Important for understanding the consistency of user behavior, identifying outliers, and assessing the reliability of averages.\n",
    "*   **Percentiles and Quantiles:** Divide the data into equal parts. Useful for segmenting users based on behavior (e.g., top 10% of engaged users) or understanding the distribution of metrics like session duration.\n",
    "\n",
    "**Takeaway for Product Analytics:** Understanding the distribution of key metrics like user engagement, retention, and conversion rates is crucial for identifying areas for product improvement and measuring the impact of changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Synthetic Data Generation\n",
    "np.random.seed(42)  # for reproducibility\n",
    "sample_size=4000\n",
    "\n",
    "# Normal distribution\n",
    "normal_data = np.random.normal(loc=15, scale=5, size=sample_size)\n",
    "normal_data = np.clip(normal_data, 0, None)\n",
    "\n",
    "# Right-skewed data\n",
    "right_skewed_data = np.random.exponential(scale=5, size=sample_size)\n",
    "right_skewed_data = np.clip(right_skewed_data, 0, None)\n",
    "\n",
    "# Left-skewed data\n",
    "left_skewed_data = 20 - np.random.exponential(scale=5, size=sample_size)\n",
    "left_skewed_data = np.clip(left_skewed_data, 0, 20)\n",
    "\n",
    "data_sets = {\n",
    "    \"Normal\": normal_data,\n",
    "    \"Right-Skewed\": right_skewed_data,\n",
    "    \"Left-Skewed\": left_skewed_data\n",
    "}\n",
    "\n",
    "# Create a Pandas DataFrame to store the descriptive statistics\n",
    "summary_data = []\n",
    "\n",
    "for name, data in data_sets.items():\n",
    "    summary_data.append({\n",
    "        \"Distribution\": name,\n",
    "        \"Mean\": np.mean(data),\n",
    "        \"Median\": np.median(data),\n",
    "        \"Mode\": stats.mode(data)[0][0] if len(np.unique(data)) < 500 else \"Not well-defined\", #check to avoid errors if there are too many unique values for the mode\n",
    "        \"Range\": np.max(data) - np.min(data),\n",
    "        \"Variance\": np.var(data),\n",
    "        \"Standard Deviation\": np.std(data),\n",
    "        \"IQR\": stats.iqr(data),\n",
    "        \"Skewness\": stats.skew(data),\n",
    "        \"Kurtosis\": stats.kurtosis(data),\n",
    "        \"25th Percentile (Q1)\": np.percentile(data, 25),\n",
    "        \"50th Percentile (Median)\": np.percentile(data, 50),\n",
    "        \"75th Percentile (Q3)\": np.percentile(data, 75),\n",
    "        \"90th Percentile\": np.percentile(data, 90),\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "df_summary = df_summary.set_index(\"Distribution\").transpose()\n",
    "\n",
    "# Display the table with styling\n",
    "# styled_df = df_summary.style.background_gradient(cmap='viridis', axis=1) # Apply gradient based on values\n",
    "display(df_summary)\n",
    "\n",
    "# Plotting the histograms and boxplots together\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))  # 3 rows (one for each distribution), 2 columns (hist and box)\n",
    "\n",
    "for i, (name, data) in enumerate(data_sets.items()):\n",
    "    sns.histplot(data, kde=True, ax=axes[i, 0], color=f'C{i}')  # Use different colors\n",
    "    axes[i, 0].set_title(f\"Distribution of {name} Data\")\n",
    "    axes[i, 0].set_xlabel(\"Value\")\n",
    "    axes[i, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "    sns.boxplot(y=data, ax=axes[i, 1], color=f'C{i}')\n",
    "    axes[i, 1].set_title(f\"Boxplot of {name} Data\")\n",
    "    axes[i, 1].set_ylabel(\"Value\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Probability Distributions <a name=\"probability-distributions\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Probability distributions describe the likelihood of different outcomes. In product analytics, they help model user behavior and predict future events. Common distributions include:\n",
    "\n",
    "*   **Normal Distribution:** Often used to model continuous variables like user engagement time or purchase amounts when data is symmetrically distributed around a mean.\n",
    "*   **Poisson Distribution:** Used to model count data, such as the number of likes, comments, or shares a post receives.\n",
    "*   **Exponential Distribution:** Used to model the time between events, such as the time between user logins or the time until a user makes a purchase.\n",
    "\n",
    "**Takeaway for Product Analytics:** Understanding which distribution best fits the data allows for more accurate analysis, prediction, and hypothesis testing. For example, knowing the distribution of user session length can help optimize server capacity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# --- Normal Distribution ---\n",
    "print(\"\\n--- 1. Normal Distribution ---\")\n",
    "mu = 0  # Mean\n",
    "sigma = 1  # Standard deviation\n",
    "x = np.linspace(stats.norm.ppf(0.001, loc=mu, scale=sigma), stats.norm.ppf(0.999, loc=mu, scale=sigma), 100)\n",
    "normal_data = np.random.normal(loc=mu, scale=sigma, size=1000)\n",
    "\n",
    "# Plot Normal Distribution PDF\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, stats.norm.pdf(x, loc=mu, scale=sigma), 'r-', lw=2, label='PDF')\n",
    "plt.title(\"Normal Distribution PDF (μ=0, σ=1)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Normal Distribution Histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.axvline(mu, color='b', linestyle='dashed', linewidth=1, label=f'Mean: {mu:.2f}')\n",
    "plt.axvline(np.median(normal_data), color='g', linestyle='dashed', linewidth=1, label=f'Median: {np.median(normal_data):.2f}')\n",
    "sns.histplot(normal_data, kde=True, alpha=0.5, label=\"Generated Data\")\n",
    "plt.title(\"Histogram of Normally Distributed Data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.text(0.5, -0.2, \"Histogram of generated data from a normal distribution. The mean and median are also shown as dashed lines.\", ha='center', va='top', transform=plt.gca().transAxes)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Poisson Distribution ---\n",
    "print(\"\\n--- 2. Poisson Distribution ---\")\n",
    "lam = 3  # Average rate (lambda)\n",
    "x_poisson = np.arange(0, stats.poisson.ppf(0.999, mu=lam) + 1)\n",
    "poisson_data = np.random.poisson(lam=lam, size=1000)\n",
    "\n",
    "# Plot Poisson Distribution PMF\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x_poisson, stats.poisson.pmf(x_poisson, mu=lam), 'bo', ms=8, label='PMF')\n",
    "plt.vlines(x_poisson, 0, stats.poisson.pmf(x_poisson, mu=lam), colors='b', lw=2, alpha=0.5)\n",
    "plt.title(f\"Poisson Distribution PMF (λ={lam})\")\n",
    "plt.xlabel(\"Number of Events (k)\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Poisson Distribution Histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.axvline(lam, color='b', linestyle='dashed', linewidth=1, label=f'Mean: {lam:.2f}')\n",
    "plt.axvline(np.median(poisson_data), color='g', linestyle='dashed', linewidth=1, label=f'Median: {np.median(poisson_data):.2f}')\n",
    "sns.histplot(poisson_data, discrete=True, alpha=0.5, label=\"Generated Data\")\n",
    "plt.title(f\"Histogram of Poisson Distributed Data\")\n",
    "plt.xlabel(\"Number of Events (k)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.text(0.5, -0.2, \"Histogram of generated Poisson data. The mean and median are shown as dashed lines.\", ha='center', va='top', transform=plt.gca().transAxes)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exponential Distribution ---\n",
    "print(\"\\n--- 3. Exponential Distribution ---\")\n",
    "scale = 2  # Scale (inverse of rate lambda)\n",
    "x_expon = np.linspace(stats.expon.ppf(0.001, scale=scale), stats.expon.ppf(0.999, scale=scale), 100)\n",
    "exponential_data = np.random.exponential(scale=scale, size=1000)\n",
    "\n",
    "# Plot Exponential Distribution PDF\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x_expon, stats.expon.pdf(x_expon, scale=scale), 'g-', lw=2, label='PDF')\n",
    "plt.title(f\"Exponential Distribution PDF (Scale={scale})\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Exponential Distribution Histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.axvline(scale, color='b', linestyle='dashed', linewidth=1, label=f'Mean: {scale:.2f}')\n",
    "plt.axvline(np.median(exponential_data), color='g', linestyle='dashed', linewidth=1, label=f'Median: {np.median(exponential_data):.2f}')\n",
    "sns.histplot(exponential_data, kde=True, alpha=0.5, label=\"Generated Data\")\n",
    "plt.title(f\"Histogram of Exponentially Distributed Data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.text(0.5, -0.2, \"Histogram of generated exponential data. The mean and median are represented by dashed lines.\", ha='center', va='top', transform=plt.gca().transAxes)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Hypothesis Testing <a name=\"hypothesis-testing\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Hypothesis testing is a formal process for evaluating evidence for or against a claim about a population based on sample data. In product analytics, it is used to determine if changes to a product have a statistically significant impact on key metrics. Key concepts include:\n",
    "\n",
    "*   **Null and Alternative Hypotheses:** The null hypothesis is the default assumption (e.g., \"the new feature has no effect\"). The alternative hypothesis is what we want to test (e.g., \"the new feature increases engagement\").\n",
    "*   **P-values and Significance Levels:** The p-value measures the probability of observing the data (or more extreme data) if the null hypothesis is true. If the p-value is below a chosen significance level (e.g., 0.05), we reject the null hypothesis.\n",
    "*   **Type I and Type II Errors:** Type I error (false positive) is rejecting the null hypothesis when it's true. Type II error (false negative) is failing to reject the null hypothesis when it's false.\n",
    "\n",
    "**Takeaway for Product Analytics:** Hypothesis testing provides a rigorous framework for evaluating the impact of product changes and making data-driven decisions.\n",
    "\n",
    "```python\n",
    "# Code implementation will go here\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. A/B Testing <a name=\"ab-testing\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "A/B testing is a specific application of hypothesis testing in product development. It involves comparing two versions of a product or feature (A and B) to see which performs better. Key considerations include:\n",
    "\n",
    "*   **Randomization:** Users are randomly assigned to either the control group (A) or the treatment group (B).\n",
    "*   **Metrics:** Key metrics are tracked for both groups to measure the impact of the change.\n",
    "*   **Statistical Significance:** Statistical tests are used to determine if the observed differences between the groups are statistically significant.\n",
    "\n",
    "**Takeaway for Product Analytics:** A/B testing is crucial for making data-driven decisions about product changes and optimizing user experience.\n",
    "\n",
    "```python\n",
    "# Code implementation will go here\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. Correlation <a name=\"correlation\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Correlation measures the strength and direction of a linear relationship between two variables. In product analytics, it can be used to identify relationships between user behavior and product features.\n",
    "\n",
    "*   **Pearson Correlation:** Measures the linear relationship between two continuous variables.\n",
    "*   **Spearman Correlation:** Measures the monotonic relationship between two variables (whether they tend to move in the same or opposite directions, not necessarily linearly).\n",
    "\n",
    "**Takeaway for Product Analytics:** Understanding correlations can help identify potential drivers of user behavior and inform product development. However, correlation does not imply causation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "data_path = os.path.join(os.getcwd(), 'data', 'hotel_bookings.csv')\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    print(\"\\n--- Correlation Analysis on Hotel Booking Demand Data ---\")\n",
    "    print(\"\\nDataset Description:\")\n",
    "    print(\"This dataset contains booking information for a city hotel and a resort hotel. We will be analyzing the relationships between some of the numerical features related to booking demand.\")\n",
    "\n",
    "    # Select numerical features for correlation analysis. Handling missing values is crucial.\n",
    "    numerical_features = ['lead_time', 'arrival_date_week_number', 'stays_in_weekend_nights',\n",
    "                          'stays_in_week_nights', 'adults', 'children', 'babies', 'previous_cancellations',\n",
    "                          'previous_bookings_not_canceled', 'booking_changes', 'days_in_waiting_list', 'adr', 'required_car_parking_spaces', 'total_of_special_requests']\n",
    "\n",
    "    df_numerical = df[numerical_features].copy() #make copy to avoid warnings\n",
    "    #Handle missing values in children by filling with 0.\n",
    "    df_numerical['children'].fillna(0, inplace=True)\n",
    "    #Handle missing values in agent and company by filling with 0.\n",
    "    df['agent'].fillna(0, inplace=True)\n",
    "    df['company'].fillna(0, inplace=True)\n",
    "\n",
    "    # Calculate Pearson correlation\n",
    "    pearson_corr = df_numerical.corr(method='pearson')\n",
    "    print(\"\\nPearson Correlation Matrix:\")\n",
    "    print(pearson_corr.to_string())\n",
    "\n",
    "    # Calculate Spearman correlation\n",
    "    spearman_corr = df_numerical.corr(method='spearman')\n",
    "    # print(\"\\nSpearman Correlation Matrix:\")\n",
    "    # print(spearman_corr.to_string())\n",
    "\n",
    "    # Visualize correlation matrix (using heatmap)\n",
    "    # plt.figure(figsize=(14, 12))  # Increased figure size for better readability\n",
    "    # sns.heatmap(pearson_corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    # plt.title(\"Pearson Correlation Heatmap\")\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    print(\"\\nCorrelation Analysis Interpretation:\")\n",
    "    print(\"Here are some observations based on the correlation matrices:\")\n",
    "\n",
    "    print(\"\\nStays and Nights:\")\n",
    "    print(\"- There is a strong positive correlation between `stays_in_weekend_nights` and `stays_in_week_nights`, as expected. Longer stays naturally involve more nights of both types.\")\n",
    "\n",
    "    print(\"\\nAdults, Children, and Babies:\")\n",
    "    print(\"- There are positive correlations between `adults`, `children`, and `babies`, indicating that bookings with more adults tend to also have more children and babies.\")\n",
    "\n",
    "    print(\"\\nLead Time and Previous Cancellations:\")\n",
    "    print(\"- There is a positive correlation between `lead_time` and `previous_cancellations`. This suggests that bookings made further in advance are more likely to be canceled.\")\n",
    "\n",
    "    print(\"\\nADR and other features:\")\n",
    "    print(\"- There is a weak positive correlation between `adr` (Average Daily Rate) and `adults`, indicating that bookings with more adults tend to have slightly higher room rates.\")\n",
    "\n",
    "    print(\"\\nImportant Note:\")\n",
    "    print(\"Correlation does not imply causation. These relationships are suggestive, not definitive. Further analysis and domain knowledge are needed to establish causality.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {data_path}. Please ensure the file exists in the 'data' folder in the same directory as the notebook.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(f\"Error: Could not parse the CSV file at {data_path}. Please check the file format.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Linear Regression <a name=\"linear-regression\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Linear regression models the linear relationship between a dependent variable and one or more independent variables. In product analytics, it can be used to predict user behavior or the impact of product changes on key metrics.\n",
    "\n",
    "**Takeaway for Product Analytics:** Linear regression can provide insights into how different factors influence user behavior and can be used for forecasting and optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate some synthetic data for demonstration\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X = np.random.rand(n_samples) * 10  # Independent variable\n",
    "true_slope = 2\n",
    "true_intercept = 5\n",
    "Y = true_slope * X + true_intercept + np.random.normal(0, 2, n_samples)  # Dependent variable with noise\n",
    "\n",
    "# Add a constant for the intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = sm.OLS(Y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# Print results\n",
    "print(results.summary())\n",
    "\n",
    "#Plot the data and regression line\n",
    "x_values = np.linspace(0,10, 100)\n",
    "x_values_const= sm.add_constant(x_values)\n",
    "predicted_values= results.predict(x_values_const)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:, 1], Y, label=\"Data Points\") #X is array with constant, plot the second column\n",
    "plt.plot(x_values, predicted_values, label=\"Regression Line\", color='red')\n",
    "plt.xlabel(\"Independent Variable\")\n",
    "plt.ylabel(\"Dependent Variable\")\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Logistic Regression <a name=\"logistic-regression\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Logistic regression is used for binary classification problems, where the dependent variable is categorical (e.g., click/no-click, convert/not-convert). In product analytics, it can be used to predict the likelihood of a user performing a specific action.\n",
    "\n",
    "**Takeaway for Product Analytics:** Logistic regression can be used to predict user churn, conversion rates, and other binary outcomes, allowing for targeted interventions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create some synthetic data for demonstration\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 200\n",
    "\n",
    "X = np.random.rand(n_samples, 2) # two features\n",
    "# Create some non-linearity for a more interesting classification\n",
    "y = (X[:, 0] + X[:, 1] + np.random.normal(0, 0.3, n_samples) > 1).astype(int)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Fit logistic regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Plot decision boundary (for two features)\n",
    "x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdBu)\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.RdBu)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Logistic Regression Decision Boundary\")\n",
    "plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Experimental Design in Social Networks <a name=\"experimental-design-in-social-networks\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Conducting experiments in social networks presents unique challenges due to network effects. Users are interconnected, which can lead to interference between treatment and control groups. Key considerations include:\n",
    "\n",
    "*   **Network Effects:** The behavior of one user can influence the behavior of their connections.\n",
    "*   **Spillover Effects:** The treatment applied to one user can \"spill over\" and affect users in the control group.\n",
    "*   **Clustering:** Users tend to cluster with similar users, which can make it difficult to achieve proper randomization.\n",
    "\n",
    "**Takeaway for Product Analytics:** It’s important to account for these challenges when designing experiments in social networks to ensure valid and reliable results. Techniques like cluster randomization or graph-based experiments can help mitigate these issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Simulating a small social network\n",
    "np.random.seed(42)\n",
    "num_users = 50\n",
    "connectivity_prob = 0.2 # Probability of connection between two users\n",
    "user_ids = list(range(num_users))\n",
    "\n",
    "# Create an adjacency matrix\n",
    "adjacency_matrix = np.random.binomial(1, connectivity_prob, size=(num_users, num_users))\n",
    "np.fill_diagonal(adjacency_matrix, 0) # Remove self-loops\n",
    "network = pd.DataFrame(adjacency_matrix, columns=user_ids, index=user_ids)\n",
    "print(\"Simulated Social Network (Adjacency Matrix):\")\n",
    "display(network)\n",
    "\n",
    "# Assign users to treatment and control groups (with basic randomization)\n",
    "treatment_size = int(num_users/2)\n",
    "treatment_group = random.sample(user_ids, treatment_size)\n",
    "control_group = [user for user in user_ids if user not in treatment_group]\n",
    "\n",
    "print(f\"\\nTreatment Group (Basic Randomization): {treatment_group}\")\n",
    "print(f\"Control Group (Basic Randomization): {control_group}\")\n",
    "\n",
    "\n",
    "# Simulate basic user behavior (simple metrics)\n",
    "def simulate_metric(group, network, treatment=False):\n",
    "    metrics = {}\n",
    "    for user in group:\n",
    "        base_metric = np.random.normal(loc=5, scale=2)\n",
    "        if treatment:\n",
    "            base_metric += np.random.normal(loc=2, scale=0.5) # Treatment Effect\n",
    "            for neighbor in network.loc[user][network.loc[user]==1].index: #Network/Spillover effect\n",
    "                if neighbor in group:\n",
    "                    base_metric += np.random.normal(loc=0.5, scale =0.25)\n",
    "        metrics[user] = base_metric\n",
    "    return metrics\n",
    "\n",
    "#Simulate with basic randomization\n",
    "control_metrics = simulate_metric(control_group, network)\n",
    "treatment_metrics = simulate_metric(treatment_group, network, treatment=True)\n",
    "\n",
    "\n",
    "# Print average metrics\n",
    "print(\"\\nAverage Metrics (Basic Randomization)\")\n",
    "print(f\"Average control metrics: {np.mean(list(control_metrics.values())):.2f}\")\n",
    "print(f\"Average treatment metrics: {np.mean(list(treatment_metrics.values())):.2f}\")\n",
    "\n",
    "#Cluster randomization\n",
    "num_clusters = 5\n",
    "#Randomly assign users to clusters\n",
    "user_cluster_assignments= {}\n",
    "for user in user_ids:\n",
    "    user_cluster_assignments[user] = random.randint(0,num_clusters -1)\n",
    "\n",
    "#randomly assing the clusters to treatment or control\n",
    "cluster_ids = list(range(num_clusters))\n",
    "treatment_clusters = random.sample(cluster_ids, int(num_clusters/2))\n",
    "control_clusters = [cluster for cluster in cluster_ids if cluster not in treatment_clusters]\n",
    "\n",
    "#assign users to treatment and control\n",
    "treatment_group = [user for user, cluster in user_cluster_assignments.items() if cluster in treatment_clusters]\n",
    "control_group = [user for user, cluster in user_cluster_assignments.items() if cluster in control_clusters]\n",
    "\n",
    "print(f\"\\nTreatment Group (Cluster Randomization): {treatment_group}\")\n",
    "print(f\"Control Group (Cluster Randomization): {control_group}\")\n",
    "\n",
    "#Simulate with cluster randomization\n",
    "control_metrics_cluster = simulate_metric(control_group, network)\n",
    "treatment_metrics_cluster = simulate_metric(treatment_group, network, treatment=True)\n",
    "\n",
    "\n",
    "# Print average metrics for cluster randomization\n",
    "print(\"\\nAverage Metrics (Cluster Randomization)\")\n",
    "print(f\"Average control metrics: {np.mean(list(control_metrics_cluster.values())):.2f}\")\n",
    "print(f\"Average treatment metrics: {np.mean(list(treatment_metrics_cluster.values())):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Bias-Variance Tradeoff <a name=\"bias-variance-tradeoff\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The bias-variance tradeoff is a fundamental concept in machine learning. It describes the relationship between a model's ability to fit the training data (low bias) and its ability to generalize to new data (low variance).\n",
    "\n",
    "*   **Bias:** Error from simplifying assumptions in the model. High bias leads to underfitting.\n",
    "*   **Variance:** Error from sensitivity to small fluctuations in the training data. High variance leads to overfitting.\n",
    "\n",
    "**Takeaway for Product Analytics:** Finding the right balance between bias and variance is crucial for building models that accurately predict user behavior and generalize well to new data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate some synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 1, 50)\n",
    "Y = np.sin(2 * np.pi * X) + np.random.normal(0, 0.2, 50)\n",
    "X= X.reshape(-1, 1)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "degrees = [1, 3, 10]\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    # Polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "\n",
    "    # Fit the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test_poly)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    # Plot the fitted function\n",
    "    X_plot = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "    X_plot_poly = poly.transform(X_plot)\n",
    "    y_plot = model.predict(X_plot_poly)\n",
    "\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.scatter(X_train, y_train, label=\"Train Data\")\n",
    "    plt.scatter(X_test, y_test, label=\"Test Data\")\n",
    "    plt.plot(X_plot, y_plot, color='red', label=\"Model Fit\")\n",
    "    plt.title(f\"Degree {degree}, MSE={mse:.3f}\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Resampling Techniques <a name=\"resampling-techniques\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Resampling techniques are used to estimate the sampling distribution of a statistic or to evaluate the performance of a model. Common techniques include:\n",
    "\n",
    "*   **Bootstrapping:** Randomly sampling with replacement from the original data to create multiple datasets and estimate the variability of a statistic (e.g., confidence intervals). Useful when the underlying distribution is unknown or complex.\n",
    "*   **Cross-Validation:** Partitioning the data into subsets and training/evaluating the model on different combinations of these subsets. Used to assess model performance and prevent overfitting. K-fold cross-validation is a common technique.\n",
    "\n",
    "**Takeaway for Product Analytics:** Resampling techniques provide robust methods for evaluating model performance and estimating uncertainty, especially when dealing with limited data or complex models. They can help in making more reliable product decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Bootstrapping ---\n",
    "print(\"--- Bootstrapping ---\")\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(loc=5, scale=2, size=100)\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Bootstrap sampling and calculate the mean\n",
    "bootstrap_means = []\n",
    "for _ in range(n_bootstraps):\n",
    "    bootstrap_sample = np.random.choice(data, size=len(data), replace=True)\n",
    "    bootstrap_means.append(np.mean(bootstrap_sample))\n",
    "\n",
    "# Estimate the mean and confidence interval\n",
    "mean_estimate = np.mean(bootstrap_means)\n",
    "std_estimate = np.std(bootstrap_means)\n",
    "confidence_interval = (mean_estimate - 1.96 * std_estimate, mean_estimate + 1.96 * std_estimate)\n",
    "\n",
    "print(f\"Estimated Mean: {mean_estimate:.3f}\")\n",
    "print(f\"95% Confidence Interval: {confidence_interval[0]:.3f}, {confidence_interval[1]:.3f}\")\n",
    "\n",
    "# Plot histogram of bootstrap means\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(bootstrap_means, bins=30, alpha=0.7)\n",
    "plt.axvline(mean_estimate, color=\"red\", linestyle=\"dashed\", label=f\"Mean: {mean_estimate:.2f}\")\n",
    "plt.axvline(confidence_interval[0], color=\"green\", linestyle=\"dashed\", label=f\"95% CI\")\n",
    "plt.axvline(confidence_interval[1], color=\"green\", linestyle=\"dashed\")\n",
    "plt.xlabel(\"Bootstrapped Mean Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Bootstrapped Means\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- K-Fold Cross-Validation ---\n",
    "print(\"\\n--- K-Fold Cross-Validation ---\")\n",
    "\n",
    "# Generate sample data for linear regression\n",
    "X = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "Y = 2 * X.squeeze() + 1 + np.random.normal(0, 0.5, 100)\n",
    "\n",
    "# Number of folds\n",
    "k = 5\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store scores for each fold\n",
    "mse_scores = []\n",
    "\n",
    "# Iterate over the folds\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    # Train the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the MSE\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "# Print average MSE\n",
    "print(f\"Mean MSE across {k} folds: {np.mean(mse_scores):.3f}\")\n",
    "\n",
    "#Plot predicted vs actual\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X, Y, label=\"Actual Data\")\n",
    "X_plot = np.linspace(0,1,100).reshape(-1,1)\n",
    "y_plot = model.predict(X_plot)\n",
    "plt.plot(X_plot, y_plot, color='red', label=\"Regression Fit\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"K-Fold CV regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Measures of Central Tendency <a name=\"measures-of-central-tendency\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Measures of central tendency describe the \"typical\" or \"average\" value in a dataset. Understanding these measures is fundamental for summarizing user behavior and identifying trends.\n",
    "\n",
    "*   **Mean:** The average of all values. Sensitive to outliers.\n",
    "*   **Median:** The middle value when the data is sorted. Robust to outliers.\n",
    "*   **Mode:** The most frequent value. Useful for categorical data or identifying peaks in distributions.\n",
    "\n",
    "**Takeaway for Product Analytics:** Choosing the appropriate measure of central tendency depends on the data's distribution and the presence of outliers. For example, the median is often preferred over the mean when analyzing user spending data, which is often skewed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data\n",
    "data = np.array([10, 12, 15, 18, 20, 20, 25, 100])\n",
    "\n",
    "# Calculate the mean\n",
    "mean_value = np.mean(data)\n",
    "print(f\"Mean: {mean_value:.2f}\")\n",
    "\n",
    "# Calculate the median\n",
    "median_value = np.median(data)\n",
    "print(f\"Median: {median_value:.2f}\")\n",
    "\n",
    "# Calculate the mode\n",
    "mode_value = stats.mode(data, keepdims=True)[0][0]\n",
    "print(f\"Mode: {mode_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 12. Measures of Dispersion <a name=\"measures-of-dispersion\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Measures of dispersion describe the spread or variability of the data. They are crucial for understanding the consistency of user behavior and identifying outliers.\n",
    "\n",
    "*   **Range:** The difference between the maximum and minimum values. Sensitive to outliers.\n",
    "*   **Variance:** The average squared deviation from the mean.\n",
    "*   **Standard Deviation:** The square root of the variance. Easier to interpret than variance as it's in the same units as the data.\n",
    "*   **Interquartile Range (IQR):** The difference between the 75th and 25th percentiles. Robust to outliers.\n",
    "\n",
    "**Takeaway for Product Analytics:** Understanding the dispersion of key metrics like session duration or purchase amounts can help identify user segments with different behavior patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data\n",
    "data = np.array([10, 12, 15, 18, 20, 20, 25, 100])\n",
    "\n",
    "# Calculate the range\n",
    "range_value = np.max(data) - np.min(data)\n",
    "print(f\"Range: {range_value}\")\n",
    "\n",
    "# Calculate the variance\n",
    "variance_value = np.var(data)\n",
    "print(f\"Variance: {variance_value:.2f}\")\n",
    "\n",
    "# Calculate the standard deviation\n",
    "std_dev_value = np.std(data)\n",
    "print(f\"Standard Deviation: {std_dev_value:.2f}\")\n",
    "\n",
    "# Calculate the IQR\n",
    "iqr_value = stats.iqr(data)\n",
    "print(f\"IQR: {iqr_value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Percentiles and Quantiles <a name=\"percentiles-and-quantiles\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Percentiles and quantiles divide the data into equal parts. They are useful for understanding the distribution of data and identifying specific thresholds.\n",
    "\n",
    "*   **Percentiles:** Divide the data into 100 equal parts. For example, the 90th percentile is the value below which 90% of the data falls.\n",
    "*   **Quartiles:** Divide the data into four equal parts (25th, 50th, and 75th percentiles).\n",
    "\n",
    "**Takeaway for Product Analytics:** Percentiles and quantiles can be used for user segmentation (e.g., top 10% of users based on engagement), setting thresholds for alerts (e.g., identifying users with unusually high activity), and understanding the distribution of key metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([10, 12, 15, 18, 20, 20, 25, 100])\n",
    "\n",
    "# Calculate percentiles\n",
    "percentiles = [25, 50, 75, 90]\n",
    "percentile_values = np.percentile(data, percentiles)\n",
    "for p, v in zip(percentiles, percentile_values):\n",
    "  print(f\"{p}th Percentile: {v:.2f}\")\n",
    "\n",
    "# Calculate quantiles (using percentiles/100)\n",
    "quantile_values = np.quantile(data, [0.25, 0.5, 0.75])\n",
    "print(f\"\\nQuartiles: Q1={quantile_values[0]:.2f}, Q2={quantile_values[1]:.2f}, Q3={quantile_values[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 14. Data Transformation <a name=\"data-transformation\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Data transformation techniques are used to change the distribution or scale of data. This can be useful for improving model performance or making data easier to interpret.\n",
    "\n",
    "*   **Standardization (Z-score):** Transforms data to have a mean of 0 and a standard deviation of 1. Useful for comparing variables with different scales.\n",
    "*   **Normalization (Min-Max scaling):** Scales data to a specific range (e.g., 0 to 1). Useful when the data has a bounded range.\n",
    "*   **Log Transformation:** Compresses the scale of data, often used to reduce skewness and make data more normally distributed.\n",
    "\n",
    "**Takeaway for Product Analytics:** Data transformations can improve the performance of machine learning models and make data easier to visualize and interpret. For example, log transformation is often used to handle skewed metrics like user spending.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sample data\n",
    "data = np.array([1, 2, 3, 4, 5, 10, 15, 20])\n",
    "data_skew = np.random.exponential(scale=2, size=200) #Example of skewed data\n",
    "\n",
    "# --- Standardization (Z-score) ---\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(data.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(\"Standardized Data:\", standardized_data)\n",
    "\n",
    "# --- Normalization (Min-Max Scaling) ---\n",
    "min_max_scaler = MinMaxScaler()\n",
    "normalized_data = min_max_scaler.fit_transform(data.reshape(-1, 1)).flatten()\n",
    "print(\"Normalized Data:\", normalized_data)\n",
    "\n",
    "# --- Log Transformation ---\n",
    "log_transformed_data = np.log1p(data_skew)\n",
    "\n",
    "#Plot distributions before/after\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data_skew, kde=True, label = 'Original Skewed Data')\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Original Skewed Data\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(log_transformed_data, kde=True, label=\"Log Transformed Data\")\n",
    "plt.xlabel(\"Log Transformed Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Log Transformed Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 15. Probability Basics <a name=\"probability-basics\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Understanding basic probability concepts is essential for data analysis and decision-making.\n",
    "\n",
    "*   **Conditional Probability:** The probability of an event occurring given that another event has already occurred.\n",
    "*   **Bayes' Theorem:** Relates conditional probabilities. Useful for updating beliefs based on new evidence.\n",
    "\n",
    "**Takeaway for Product Analytics:** Conditional probability can be used to understand the likelihood of a user performing an action given their past behavior. Bayes' theorem can be used to update estimates of conversion rates or other metrics based on A/B test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional Probability Example\n",
    "\n",
    "# Define events: A is clicking an ad, B is making a purchase\n",
    "#P(B|A) - Probability of making a purchase given user clicked the ad\n",
    "P_A = 0.1  # Probability of clicking an ad\n",
    "P_B = 0.05  # Probability of making a purchase\n",
    "P_A_and_B = 0.02 # Probability of both\n",
    "\n",
    "P_B_given_A = P_A_and_B/P_A\n",
    "\n",
    "print(f\"P(A): {P_A}\")\n",
    "print(f\"P(B): {P_B}\")\n",
    "print(f\"P(A and B): {P_A_and_B}\")\n",
    "print(f\"P(B|A) - Probability of making a purchase given click {P_B_given_A:.2f}\")\n",
    "\n",
    "# Bayes Theorem\n",
    "# P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "P_A_given_B = (P_B_given_A * P_A)/P_B\n",
    "\n",
    "print(f\"P(A|B) Probability of a user clicking an ad given they purchase: {P_A_given_B:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Discrete Distributions <a name=\"discrete-distributions\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Discrete distributions describe the probability of discrete outcomes (e.g., counts).\n",
    "\n",
    "*   **Bernoulli Distribution:** Models a single binary outcome (e.g., click/no-click).\n",
    "*   **Binomial Distribution:** Models the number of successes in a fixed number of independent Bernoulli trials.\n",
    "*   **Poisson Distribution:** Models the number of events occurring in a fixed interval of time or space.\n",
    "\n",
    "**Takeaway for Product Analytics:** Discrete distributions can be used to model user behavior such as conversion rates (Binomial), number of support tickets (Poisson), etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Bernoulli Distribution ---\n",
    "p = 0.3 # probability of success\n",
    "bern_data = np.random.binomial(1, p, size = 1000)\n",
    "print(\"Bernoulli Distribution Simulation:\")\n",
    "print(f\"Mean of data: {np.mean(bern_data):.2f}, Expected: {p}\")\n",
    "\n",
    "# --- Binomial Distribution ---\n",
    "n = 10 # number of trials\n",
    "p = 0.4 # probability of success\n",
    "binomial_data = np.random.binomial(n, p, size=1000)\n",
    "print(\"\\nBinomial Distribution Simulation:\")\n",
    "print(f\"Mean of data: {np.mean(binomial_data):.2f}, Expected Mean: {n*p}\")\n",
    "\n",
    "# --- Poisson Distribution ---\n",
    "lam = 5 # average rate (lambda)\n",
    "poisson_data = np.random.poisson(lam, size = 1000)\n",
    "\n",
    "print(\"\\nPoisson Distribution Simulation:\")\n",
    "print(f\"Mean of data: {np.mean(poisson_data):.2f}, Expected Mean: {lam}\")\n",
    "\n",
    "# Plot Histograms\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(bern_data, discrete=True, label=\"Bernoulli Data\")\n",
    "plt.title(\"Bernoulli Distribution Simulation\")\n",
    "plt.xlabel(\"Outcomes\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(binomial_data, discrete=True, label=\"Binomial Data\")\n",
    "plt.title(\"Binomial Distribution Simulation\")\n",
    "plt.xlabel(\"Number of Successes\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.histplot(poisson_data, discrete=True, label=\"Poisson Data\")\n",
    "plt.title(\"Poisson Distribution Simulation\")\n",
    "plt.xlabel(\"Number of Events\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Continuous Distributions <a name=\"continuous-distributions\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Continuous distributions describe the probability of continuous outcomes (e.g., time, weight, height).\n",
    "\n",
    "*   **Normal Distribution:** Often used to model real-valued random variables whose distributions are not known. Many natural phenomena approximately follow a normal distribution.\n",
    "*   **Exponential Distribution:** Often used to model the time between events in a Poisson process.\n",
    "*   **Uniform Distribution:** All outcomes within a given range are equally likely.\n",
    "\n",
    "**Takeaway for Product Analytics:** Continuous distributions can be used to model metrics like session duration, purchase amounts, and other continuous user behavior data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# --- Normal Distribution ---\n",
    "mu = 0 # Mean\n",
    "sigma = 1 # Standard Deviation\n",
    "normal_data = np.random.normal(mu, sigma, size=1000)\n",
    "print(f\"Normal Distribution Simulated Data Mean: {np.mean(normal_data):.2f}, Std. Dev: {np.std(normal_data):.2f}\")\n",
    "\n",
    "\n",
    "# --- Exponential Distribution ---\n",
    "scale = 2 # Scale\n",
    "exponential_data = np.random.exponential(scale, size = 1000)\n",
    "print(f\"\\nExponential Distribution Simulated Data Mean: {np.mean(exponential_data):.2f}, Expected: {scale}\")\n",
    "\n",
    "# --- Uniform Distribution ---\n",
    "low = 1 # Lower bound\n",
    "high = 10 # Upper Bound\n",
    "uniform_data = np.random.uniform(low, high, size=1000)\n",
    "print(f\"\\nUniform Distribution Simulated Data Mean: {np.mean(uniform_data):.2f}, Expected Midpoint: {(low+high)/2}\")\n",
    "\n",
    "#Plot distributions\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(normal_data, kde=True, label = \"Normal Data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Normal Distribution\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(exponential_data, kde=True, label =\"Exponential Data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Exponential Distribution\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(uniform_data, kde=True, label = \"Uniform Data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Uniform Distribution\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 18. Central Limit Theorem <a name=\"central-limit-theorem\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The Central Limit Theorem (CLT) states that the distribution of sample means will approach a normal distribution as the sample size increases, regardless of the shape of the original population distribution.\n",
    "\n",
    "**Takeaway for Product Analytics:** The CLT is fundamental for statistical inference. It allows us to use normal distribution-based tests (like t-tests and z-tests) even when the underlying population distribution is not normal, provided we have a sufficiently large sample size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Population distribution (example using exponential)\n",
    "np.random.seed(42)\n",
    "population_data = np.random.exponential(scale=5, size=10000)\n",
    "\n",
    "\n",
    "# Number of samples and sample sizes\n",
    "n_samples = 1000\n",
    "sample_sizes = [3, 30, 100]\n",
    "\n",
    "# Generate sample means\n",
    "sample_means = []\n",
    "for sample_size in sample_sizes:\n",
    "    sample_means_for_size = []\n",
    "    for _ in range(n_samples):\n",
    "        sample = np.random.choice(population_data, size=sample_size)\n",
    "        sample_means_for_size.append(np.mean(sample))\n",
    "    sample_means.append(sample_means_for_size)\n",
    "\n",
    "\n",
    "#Plot histograms to see distribution\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1, len(sample_sizes)+1 , 1)\n",
    "sns.histplot(population_data, kde=True, label=\"Population\")\n",
    "plt.title(\"Population Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "\n",
    "for i, sample_means_for_size in enumerate(sample_means):\n",
    "    plt.subplot(1, len(sample_sizes)+1, i+2)\n",
    "    sns.histplot(sample_means_for_size, kde=True, label=f\"Sample Size: {sample_sizes[i]}\")\n",
    "    plt.xlabel(\"Sample Mean\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Sample Means Distribution\")\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Hypothesis Testing Framework <a name=\"hypothesis-testing-framework\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The hypothesis testing framework provides a structured approach to making decisions based on data.\n",
    "\n",
    "*   **Null and Alternative Hypotheses:** Defining the hypotheses being tested.\n",
    "*   **Test Statistic:** A statistic calculated from the sample data to evaluate the hypotheses.\n",
    "*   **P-value:** The probability of observing the data (or more extreme data) if the null hypothesis is true.\n",
    "*   **Significance Level (alpha):** A threshold for rejecting the null hypothesis (typically 0.05).\n",
    "*   **Type I and Type II Errors:** Understanding the risks of making incorrect conclusions.\n",
    "\n",
    "**Takeaway for Product Analytics:** The hypothesis testing framework provides a rigorous way to evaluate the impact of product changes and avoid making decisions based on random fluctuations in data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "\n",
    "# --- Example of Hypothesis Testing Framework ---\n",
    "\n",
    "#Simulate a simple scenario: testing whether the mean of a sample is different from a population mean\n",
    "# Set Parameters:\n",
    "np.random.seed(42)\n",
    "pop_mean = 10\n",
    "sample_size = 50\n",
    "effect_size = 0.5 # the difference we would like to detect\n",
    "alpha = 0.05 #significance level, type 1 error\n",
    "power = 0.8 # probability of avoiding type 2 error\n",
    "\n",
    "# Calculate sample size with statsmodels power analysis tool\n",
    "analysis = TTestIndPower() # indendent samples t test\n",
    "sample_size = analysis.solve_power(effect_size = effect_size, power=power, alpha=alpha)\n",
    "sample_size = math.ceil(sample_size) # round up\n",
    "print(f\"Sample size needed for specified alpha, power, effect size: {sample_size}\")\n",
    "\n",
    "\n",
    "# Generate sample with an effect, to show an example where we reject the null hypothesis\n",
    "sample_mean_true = pop_mean + effect_size\n",
    "sample = np.random.normal(loc = sample_mean_true, scale = 2, size = sample_size)\n",
    "\n",
    "print(f\"\\nExample hypothesis test with the given sample, Null Hypothesis μ={pop_mean}\")\n",
    "#Perform one-sample t-test\n",
    "t_stat, p_value = stats.ttest_1samp(sample, pop_mean)\n",
    "print(f\"T Statistic: {t_stat:.3f}\")\n",
    "print(f\"P Value: {p_value:.3f}\")\n",
    "\n",
    "# Make a decision based on p-value\n",
    "if p_value < alpha:\n",
    "    print(\"Reject null hypothesis. The mean is significantly different from the population mean\")\n",
    "else:\n",
    "    print(\"Fail to reject null hypothesis. There is not a significant difference\")\n",
    "\n",
    "# Example histogram to visualize\n",
    "sns.histplot(sample, kde=True, label=\"Sample Data\")\n",
    "plt.axvline(pop_mean, color='r', linestyle='dashed', label=\"Pop. mean\")\n",
    "plt.axvline(np.mean(sample), color='g', linestyle=\"dashed\", label=\"Sample mean\")\n",
    "plt.title(\"Hypothesis Testing Framework Example\")\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 20. One-Sample Tests <a name=\"one-sample-tests\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "One-sample tests are used to compare a sample mean to a known or hypothesized population mean.\n",
    "\n",
    "*   **t-test:** Used when the population standard deviation is unknown.\n",
    "*   **z-test:** Used when the population standard deviation is known or the sample size is very large.\n",
    "\n",
    "**Takeaway for Product Analytics:** These tests can be used to determine if a metric has changed significantly over time or if a sample of users differs significantly from the general population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# --- Example of One-Sample t-test ---\n",
    "# Simulating user activity from product A\n",
    "np.random.seed(42)\n",
    "population_mean = 10 # assume that we have this value based on historical data\n",
    "sample_size = 30\n",
    "sample_product_a = np.random.normal(loc=11, scale=3, size = sample_size) #Simulate data from product A\n",
    "\n",
    "print(\"One Sample t-test:\")\n",
    "\n",
    "#Perform the t-test\n",
    "t_stat, p_value = stats.ttest_1samp(sample_product_a, population_mean)\n",
    "print(f\"T statistic: {t_stat:.3f}\")\n",
    "print(f\"P value: {p_value:.3f}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis. The mean from Product A is different from the population mean.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. The mean from Product A is not significantly different from the population mean.\")\n",
    "\n",
    "\n",
    "# --- Example of One-Sample z-test ---\n",
    "#Example with the same mean data but assume we know pop std dev\n",
    "print(\"\\nOne Sample z-test (assume we know population standard deviation):\")\n",
    "population_std = 3\n",
    "z_stat = (np.mean(sample_product_a) - population_mean)/(population_std/np.sqrt(sample_size)) # calculate z-stat\n",
    "p_value_z = 2*(1 - stats.norm.cdf(np.abs(z_stat))) # calculate two-sided p-value from z score\n",
    "\n",
    "print(f\"Z statistic: {z_stat:.3f}\")\n",
    "print(f\"P value: {p_value_z:.3f}\")\n",
    "\n",
    "if p_value_z < alpha:\n",
    "    print(\"Reject the null hypothesis. The mean from Product A is different from the population mean.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. The mean from Product A is not significantly different from the population mean.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 21. Two-Sample Tests <a name=\"two-sample-tests\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Two-sample tests are used to compare the means of two independent samples or to analyze relationships between categorical variables.\n",
    "\n",
    "*   **Independent t-test:** Used to compare the means of two independent groups (e.g., control vs. treatment in an A/B test). Assumes equal variances (or uses a Welch's t-test if variances are unequal).\n",
    "*   **Paired t-test:** Used to compare the means of two related groups (e.g., before and after measurements on the same users).\n",
    "*   **Chi-square test:** Used to test for independence between two categorical variables (e.g., ad format and conversion rate).\n",
    "\n",
    "**Takeaway for Product Analytics:** These tests are crucial for A/B testing, comparing different user segments, and analyzing relationships between categorical product features and user behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# --- Example of Independent t-test ---\n",
    "np.random.seed(42)\n",
    "group_a = np.random.normal(loc=10, scale=2, size=100)\n",
    "group_b = np.random.normal(loc=10.5, scale=2, size=100)\n",
    "print(\"Independent samples t-test:\")\n",
    "\n",
    "#Perform independent samples t-test\n",
    "t_stat, p_value = stats.ttest_ind(group_a, group_b)\n",
    "print(f\"T statistic: {t_stat:.3f}\")\n",
    "print(f\"P Value: {p_value:.3f}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis. There is a statistically significant difference between group A and group B.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. There is not a statistically significant difference between group A and group B.\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Example of Paired t-test ---\n",
    "print(\"\\nPaired samples t-test:\")\n",
    "# Generate paired samples (before and after)\n",
    "before = np.random.normal(loc=10, scale=3, size=100)\n",
    "after = before + np.random.normal(loc=0.3, scale=1, size=100) # adding an effect\n",
    "\n",
    "t_stat_paired, p_value_paired = stats.ttest_rel(before, after)\n",
    "print(f\"T statistic: {t_stat_paired:.3f}\")\n",
    "print(f\"P Value: {p_value_paired:.3f}\")\n",
    "\n",
    "if p_value_paired < alpha:\n",
    "    print(\"Reject null. The mean of before and after are different.\")\n",
    "else:\n",
    "    print(\"Fail to reject null. There is not a significant difference between before and after mean.\")\n",
    "\n",
    "# --- Example of Chi-square test ---\n",
    "print(\"\\nChi-square test:\")\n",
    "observed_counts = np.array([[30, 70], [40, 60]]) #Example of categorical conversion data\n",
    "#Example of categorical conversion data\n",
    "#                     Converted    Did not convert\n",
    "#  Variant A         30              70\n",
    "#  Variant B         40              60\n",
    "\n",
    "\n",
    "#Perform the test\n",
    "chi2_stat, p_value_chi, _, _ = stats.chi2_contingency(observed_counts)\n",
    "\n",
    "print(f\"Chi2 statistic: {chi2_stat:.3f}\")\n",
    "print(f\"P value: {p_value_chi:.3f}\")\n",
    "\n",
    "if p_value_chi < alpha:\n",
    "    print(\"Reject the null. There is an association between the groups and the event.\")\n",
    "else:\n",
    "    print(\"Fail to reject. There isn't an association between the groups and the event.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. A/B Testing in Social Networks <a name=\"ab-testing-in-social-networks\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "A/B testing in social networks requires careful consideration of network effects, where a user's behavior can influence their connections.\n",
    "\n",
    "*   **Network Effects:** The treatment applied to one user can \"spill over\" and affect users in the control group (interference).\n",
    "*   **Spillover Effects:** Measuring and mitigating spillover effects is crucial for accurate A/B test results.\n",
    "*   **Clustering:** Users tend to cluster with similar users, which can bias randomization.\n",
    "*   **Solutions:** Cluster randomization (randomizing at the group level), graph-based experiments, and careful metric selection are important considerations.\n",
    "\n",
    "**Takeaway for Product Analytics:** Ignoring network effects can lead to inaccurate conclusions in A/B tests. Specialized techniques are necessary to obtain reliable results in social network settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Simulating a small social network\n",
    "np.random.seed(42)\n",
    "num_users = 50\n",
    "connectivity_prob = 0.2 # Probability of connection between two users\n",
    "user_ids = list(range(num_users))\n",
    "\n",
    "# Create an adjacency matrix\n",
    "adjacency_matrix = np.random.binomial(1, connectivity_prob, size=(num_users, num_users))\n",
    "np.fill_diagonal(adjacency_matrix, 0) # Remove self-loops\n",
    "network = pd.DataFrame(adjacency_matrix, columns=user_ids, index=user_ids)\n",
    "print(\"Simulated Social Network (Adjacency Matrix):\")\n",
    "display(network)\n",
    "\n",
    "# Assign users to treatment and control groups (with basic randomization)\n",
    "treatment_size = int(num_users/2)\n",
    "treatment_group = random.sample(user_ids, treatment_size)\n",
    "control_group = [user for user in user_ids if user not in treatment_group]\n",
    "\n",
    "print(f\"\\nTreatment Group (Basic Randomization): {treatment_group}\")\n",
    "print(f\"Control Group (Basic Randomization): {control_group}\")\n",
    "\n",
    "\n",
    "# Simulate basic user behavior (simple metrics)\n",
    "def simulate_metric(group, network, treatment=False):\n",
    "    metrics = {}\n",
    "    for user in group:\n",
    "        base_metric = np.random.normal(loc=5, scale=2)\n",
    "        if treatment:\n",
    "            base_metric += np.random.normal(loc=2, scale=0.5) # Treatment Effect\n",
    "            for neighbor in network.loc[user][network.loc[user]==1].index: #Network/Spillover effect\n",
    "                if neighbor in group:\n",
    "                    base_metric += np.random.normal(loc=0.5, scale =0.25)\n",
    "        metrics[user] = base_metric\n",
    "    return metrics\n",
    "\n",
    "#Simulate with basic randomization\n",
    "control_metrics = simulate_metric(control_group, network)\n",
    "treatment_metrics = simulate_metric(treatment_group, network, treatment=True)\n",
    "\n",
    "\n",
    "# Print average metrics\n",
    "print(\"\\nAverage Metrics (Basic Randomization)\")\n",
    "print(f\"Average control metrics: {np.mean(list(control_metrics.values())):.2f}\")\n",
    "print(f\"Average treatment metrics: {np.mean(list(treatment_metrics.values())):.2f}\")\n",
    "\n",
    "#Cluster randomization\n",
    "num_clusters = 5\n",
    "#Randomly assign users to clusters\n",
    "user_cluster_assignments= {}\n",
    "for user in user_ids:\n",
    "    user_cluster_assignments[user] = random.randint(0,num_clusters -1)\n",
    "\n",
    "#randomly assing the clusters to treatment or control\n",
    "cluster_ids = list(range(num_clusters))\n",
    "treatment_clusters = random.sample(cluster_ids, int(num_clusters/2))\n",
    "control_clusters = [cluster for cluster in cluster_ids if cluster not in treatment_clusters]\n",
    "\n",
    "#assign users to treatment and control\n",
    "treatment_group = [user for user, cluster in user_cluster_assignments.items() if cluster in treatment_clusters]\n",
    "control_group = [user for user, cluster in user_cluster_assignments.items() if cluster in control_clusters]\n",
    "\n",
    "print(f\"\\nTreatment Group (Cluster Randomization): {treatment_group}\")\n",
    "print(f\"Control Group (Cluster Randomization): {control_group}\")\n",
    "\n",
    "#Simulate with cluster randomization\n",
    "control_metrics_cluster = simulate_metric(control_group, network)\n",
    "treatment_metrics_cluster = simulate_metric(treatment_group, network, treatment=True)\n",
    "\n",
    "\n",
    "# Print average metrics for cluster randomization\n",
    "print(\"\\nAverage Metrics (Cluster Randomization)\")\n",
    "print(f\"Average control metrics: {np.mean(list(control_metrics_cluster.values())):.2f}\")\n",
    "print(f\"Average treatment metrics: {np.mean(list(treatment_metrics_cluster.values())):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. Observational Studies vs. Experiments <a name=\"observational-studies-vs-experiments\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "*   **Observational Studies:** Data is collected without any intervention. Correlation can be observed, but causation cannot be inferred.\n",
    "*   **Experiments:** Involve manipulating a variable (treatment) and observing its effect on an outcome. Allow for causal inference through randomization and control.\n",
    "\n",
    "**Takeaway for Product Analytics:** While observational studies can identify interesting trends, experiments (like A/B tests) are necessary to establish causal relationships between product changes and user behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# --- Example of Observational Study (Correlation) ---\n",
    "print(\"Example Observational Study:\")\n",
    "# Generate some observational data\n",
    "np.random.seed(42)\n",
    "num_users = 100\n",
    "hours_spent = np.random.uniform(1, 10, num_users)\n",
    "engagement = 0.6 * hours_spent + np.random.normal(0, 2, num_users)  #Engagement is correlated with hours, but not a direct effect\n",
    "df_obs = pd.DataFrame({'Hours_Spent': hours_spent, 'Engagement': engagement})\n",
    "\n",
    "# Calculate the correlation\n",
    "corr_obs = df_obs['Hours_Spent'].corr(df_obs['Engagement'])\n",
    "print(f\"Correlation: {corr_obs:.3f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(df_obs['Hours_Spent'], df_obs['Engagement'])\n",
    "plt.xlabel(\"Hours Spent\")\n",
    "plt.ylabel(\"Engagement\")\n",
    "plt.title(\"Observational Study\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation of Observational Study:\")\n",
    "print(\"An observational study shows correlation between two variables but doesn't establish causation.\")\n",
    "print(\"In this example we see a positive relationship between hours spent and engagement, however other factors could be affecting engagement\")\n",
    "# --- Example of Experiment (A/B Test) ---\n",
    "print(\"\\nExample Experiment (A/B Test):\")\n",
    "# Generate some A/B test data\n",
    "control_group = np.random.normal(10, 2, 100)  # User engagement scores for group A\n",
    "treatment_group = np.random.normal(11, 2, 100)  # User engagement scores for group B (with treatment)\n",
    "\n",
    "# Perform the t-test to determine difference\n",
    "from scipy import stats\n",
    "t_stat, p_value = stats.ttest_ind(control_group, treatment_group)\n",
    "print(f\"T statistic: {t_stat:.3f}\")\n",
    "print(f\"P Value: {p_value:.3f}\")\n",
    "\n",
    "\n",
    "# Visualization of A/B test\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(control_group, label='Control Group', alpha=0.6)\n",
    "sns.histplot(treatment_group, label='Treatment Group', alpha=0.6)\n",
    "plt.xlabel(\"Engagement Score\")\n",
    "plt.title(\"A/B Test Experiment\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation of Experiment:\")\n",
    "print(\"An experiment (A/B test) where users were randomly assigned can show a casual relationship between a change and its effect.\")\n",
    "print(\"With statistical tests we can determine the significant differences in groups, where we would reject the null hypothesis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24. Bias-Variance Tradeoff <a name=\"bias-variance-tradeoff\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The bias-variance tradeoff describes the balance between a model's ability to fit the training data and its ability to generalize to new, unseen data.\n",
    "\n",
    "*   **Bias:** Error from simplifying assumptions in the model (underfitting).\n",
    "*   **Variance:** Error from sensitivity to small fluctuations in the training data (overfitting).\n",
    "*   **Finding the Balance:** Complex models have low bias but high variance, while simple models have high bias but low variance. The goal is to find a model with the right balance to minimize generalization error.\n",
    "\n",
    "**Takeaway for Product Analytics:** Understanding the bias-variance tradeoff helps in choosing appropriate model complexity and avoiding overfitting or underfitting, leading to more accurate predictions of user behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate some synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 1, 50)\n",
    "Y = np.sin(2 * np.pi * X) + np.random.normal(0, 0.2, 50)\n",
    "X= X.reshape(-1, 1)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "degrees = [1, 3, 10]\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    # Polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "\n",
    "    # Fit the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test_poly)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    # Plot the fitted function\n",
    "    X_plot = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "    X_plot_poly = poly.transform(X_plot)\n",
    "    y_plot = model.predict(X_plot_poly)\n",
    "\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.scatter(X_train, y_train, label=\"Train Data\")\n",
    "    plt.scatter(X_test, y_test, label=\"Test Data\")\n",
    "    plt.plot(X_plot, y_plot, color='red', label=\"Model Fit\")\n",
    "    plt.title(f\"Degree {degree}, MSE={mse:.3f}\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"Model complexity is controlled with the 'degree' parameter.\")\n",
    "print(\"When the degree is 1, the model is underfitted showing high bias, and the points don't fit the data well.\")\n",
    "print(\"When the degree is 10, the model is overfitted showing high variance, and is too sensitive to training data (resulting in poor test score).\")\n",
    "print(\"Degree 3 is the ideal, having a good balance between bias and variance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25. Resampling Techniques <a name=\"resampling-techniques\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Resampling techniques are powerful tools used to estimate the sampling distribution of a statistic or evaluate the performance of a model, especially when analytical solutions are difficult or the underlying data distribution is unknown.\n",
    "\n",
    "*   **Bootstrapping:** Involves repeatedly drawing samples *with replacement* from the original dataset to create multiple \"bootstrap\" datasets. Statistics calculated on these bootstrap datasets (e.g., mean, standard deviation, confidence intervals) provide estimates of the variability of the original statistic.\n",
    "*   **Cross-Validation (k-fold):** Partitions the dataset into *k* equally sized \"folds.\" The model is trained *k* times, each time using *k-1* folds for training and the remaining fold for validation. This provides a robust estimate of model performance on unseen data and helps prevent overfitting.\n",
    "\n",
    "**Takeaway for Product Analytics:** Resampling techniques are invaluable for assessing the reliability of metrics and model predictions. Bootstrapping can help estimate confidence intervals for key metrics like conversion rates, while cross-validation provides a more realistic estimate of model performance in real-world scenarios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Bootstrapping ---\n",
    "print(\"--- Bootstrapping ---\")\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(loc=5, scale=2, size=100)\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Bootstrap sampling and calculate the mean\n",
    "bootstrap_means = []\n",
    "for _ in range(n_bootstraps):\n",
    "    bootstrap_sample = np.random.choice(data, size=len(data), replace=True)\n",
    "    bootstrap_means.append(np.mean(bootstrap_sample))\n",
    "\n",
    "# Estimate the mean and confidence interval\n",
    "mean_estimate = np.mean(bootstrap_means)\n",
    "std_estimate = np.std(bootstrap_means)\n",
    "confidence_interval = (mean_estimate - 1.96 * std_estimate, mean_estimate + 1.96 * std_estimate)\n",
    "\n",
    "print(f\"Estimated Mean: {mean_estimate:.3f}\")\n",
    "print(f\"95% Confidence Interval: {confidence_interval[0]:.3f}, {confidence_interval[1]:.3f}\")\n",
    "\n",
    "# Plot histogram of bootstrap means\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(bootstrap_means, bins=30, alpha=0.7)\n",
    "plt.axvline(mean_estimate, color=\"red\", linestyle=\"dashed\", label=f\"Mean: {mean_estimate:.2f}\")\n",
    "plt.axvline(confidence_interval[0], color=\"green\", linestyle=\"dashed\", label=f\"95% CI\")\n",
    "plt.axvline(confidence_interval[1], color=\"green\", linestyle=\"dashed\")\n",
    "plt.xlabel(\"Bootstrapped Mean Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Bootstrapped Means\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- K-Fold Cross-Validation ---\n",
    "print(\"\\n--- K-Fold Cross-Validation ---\")\n",
    "\n",
    "# Generate sample data for linear regression\n",
    "X = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "Y = 2 * X.squeeze() + 1 + np.random.normal(0, 0.5, 100)\n",
    "\n",
    "# Number of folds\n",
    "k = 5\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store scores for each fold\n",
    "mse_scores = []\n",
    "\n",
    "# Iterate over the folds\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    # Train the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the MSE\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "# Print average MSE\n",
    "print(f\"Mean MSE across {k} folds: {np.mean(mse_scores):.3f}\")\n",
    "\n",
    "#Plot predicted vs actual\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X, Y, label=\"Actual Data\")\n",
    "X_plot = np.linspace(0,1,100).reshape(-1,1)\n",
    "y_plot = model.predict(X_plot)\n",
    "plt.plot(X_plot, y_plot, color='red', label=\"Regression Fit\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"K-Fold CV regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 26. Statistical Power and Sample Size <a name=\"statistical-power-and-sample-size\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "These concepts are crucial for designing effective experiments and ensuring that the results are statistically meaningful.\n",
    "\n",
    "*   **Statistical Power:** The probability of correctly rejecting the null hypothesis when it is false (i.e., detecting a true effect). Typically, a power of 0.8 (80%) is desired.\n",
    "*   **Sample Size:** The number of observations in a study. A larger sample size generally increases statistical power.\n",
    "*   **Relationship:** Power, sample size, effect size (the magnitude of the effect you're trying to detect), and significance level (alpha) are interrelated. Given any three, you can determine the fourth.\n",
    "\n",
    "**Takeaway for Product Analytics:** Understanding statistical power and sample size allows product analysts to design A/B tests that have a high chance of detecting meaningful changes in key metrics. It also helps avoid underpowered studies that may lead to false negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.power import TTestIndPower\n",
    "import math\n",
    "\n",
    "# Define the parameters\n",
    "alpha = 0.05  # Significance level (Type I error)\n",
    "power = 0.8   # Desired statistical power (1 - Type II error)\n",
    "effect_size = 0.3  # Estimated effect size, must be given by us (difference between two means)\n",
    "\n",
    "# Calculate sample size with statsmodels power analysis tool\n",
    "analysis = TTestIndPower() # indendent samples t test\n",
    "sample_size = analysis.solve_power(effect_size = effect_size, power=power, alpha=alpha)\n",
    "sample_size = math.ceil(sample_size) # round up\n",
    "\n",
    "print(f\"Sample Size required for each group (for alpha = {alpha}, power = {power}, effect_size = {effect_size}): {sample_size}\")\n",
    "\n",
    "# Demonstrate the relationship by calculating with different effect sizes\n",
    "effect_sizes = [0.1, 0.3, 0.5]\n",
    "print(\"\\nSample size for different effect sizes (fixed alpha and power):\")\n",
    "for eff_size in effect_sizes:\n",
    "    sample_size_eff = analysis.solve_power(effect_size = eff_size, power=power, alpha=alpha)\n",
    "    sample_size_eff = math.ceil(sample_size_eff)\n",
    "    print(f\"Effect size = {eff_size}, Sample size needed: {sample_size_eff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 27. P-values and Significance Levels <a name=\"p-values-and-significance-levels\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "These are fundamental concepts in hypothesis testing.\n",
    "\n",
    "*   **P-value:** The probability of observing the data (or more extreme data) if the null hypothesis is true. A small p-value suggests that the observed data is unlikely under the null hypothesis.\n",
    "*   **Significance Level (alpha):** A predetermined threshold (typically 0.05) used to decide whether to reject the null hypothesis. If the p-value is less than alpha, the null hypothesis is rejected.\n",
    "\n",
    "**Takeaway for Product Analytics:** P-values and significance levels provide a framework for making objective decisions based on data. It's crucial to understand the limitations of p-values and to consider the context of the problem when interpreting results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "np.random.seed(42)\n",
    "sample_a = np.random.normal(loc=10, scale=2, size=100)\n",
    "sample_b = np.random.normal(loc=10.5, scale=2, size=100)\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = stats.ttest_ind(sample_a, sample_b)\n",
    "print(f\"T Statistic: {t_stat:.3f}\")\n",
    "print(f\"P Value: {p_value:.3f}\")\n",
    "\n",
    "\n",
    "alpha = 0.05\n",
    "print(f\"\\nSignificance Level (alpha) = {alpha}\")\n",
    "if p_value < alpha:\n",
    "    print(\"Reject null hypothesis. The mean of sample B is significantly different from sample A\")\n",
    "else:\n",
    "    print(\"Fail to reject null hypothesis. There is not a significant difference between sample A and sample B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28. Confidence Intervals <a name=\"confidence-intervals\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "A confidence interval provides a range of values within which the true population parameter is likely to lie, with a certain level of confidence (e.g., 95%).\n",
    "\n",
    "**Takeaway for Product Analytics:** Confidence intervals provide a more informative way to present results than just point estimates. They give a sense of the uncertainty associated with the estimate and can be used to compare different groups or conditions. For example, in an A/B test, comparing the confidence intervals of the conversion rates for the control and treatment groups can help determine if the difference is statistically meaningful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample Data\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(loc=5, scale=2, size=100)\n",
    "\n",
    "# Calculate sample mean and standard error\n",
    "sample_mean = np.mean(data)\n",
    "sample_std_error = stats.sem(data)\n",
    "\n",
    "# Confidence Level\n",
    "confidence_level = 0.95\n",
    "\n",
    "# Calculate Margin of Error\n",
    "confidence_interval = stats.t.interval(confidence_level, len(data)-1, loc=sample_mean, scale = sample_std_error)\n",
    "\n",
    "print(f\"Sample Mean: {sample_mean:.3f}\")\n",
    "print(f\"Confidence Interval ({confidence_level * 100}%) : {confidence_interval[0]:.3f}, {confidence_interval[1]:.3f}\")\n",
    "\n",
    "# Bootstrapping example\n",
    "# Generate bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "bootstrap_means = []\n",
    "for _ in range(n_bootstraps):\n",
    "    sample = np.random.choice(data, size=len(data), replace=True)\n",
    "    bootstrap_means.append(np.mean(sample))\n",
    "\n",
    "\n",
    "# Calculate CI from bootstrapped means (2.5 and 97.5 percentiles)\n",
    "lower_bound_boot = np.percentile(bootstrap_means, 2.5)\n",
    "upper_bound_boot = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "print(f\"\\nBootstrapped Confidence Interval ({confidence_level * 100}%) : {lower_bound_boot:.3f}, {upper_bound_boot:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 29. Correlation (Pearson and Spearman) <a name=\"correlation-pearson-and-spearman\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Correlation measures the strength and direction of the relationship between two variables.\n",
    "\n",
    "*   **Pearson Correlation:** Measures the *linear* relationship between two continuous variables. Sensitive to outliers.\n",
    "*   **Spearman Correlation:** Measures the *monotonic* relationship between two variables (whether they tend to move in the same or opposite directions), regardless of linearity. More robust to outliers than Pearson correlation.\n",
    "\n",
    "**Takeaway for Product Analytics:** Correlation analysis can identify relationships between user behavior and product features. However, it's essential to remember that correlation does not imply causation.\n",
    "\n",
    "```python\n",
    "# Code implementation will go here\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 30. Linear Regression (Simple and Multiple) <a name=\"linear-regression\"></a>\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Linear regression models the linear relationship between a dependent variable and one or more independent variables.\n",
    "\n",
    "*   **Simple Linear Regression:** One independent variable.\n",
    "*   **Multiple Linear Regression:** Multiple independent variables.\n",
    "*   **Interpretation of Coefficients:** The coefficients represent the change in the dependent variable for a one-unit change in the independent variable, holding other variables constant (in multiple regression).\n",
    "*   **R-squared:** Measures the proportion of variance in the dependent variable explained by the model.\n",
    "\n",
    "**Takeaway for Product Analytics:** Linear regression can be used to predict user behavior, understand the impact of different factors on key metrics, and identify areas for product optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# --- Simple Linear Regression ---\n",
    "print(\"Simple Linear Regression Example:\")\n",
    "# Generate some data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100) * 10  # Independent variable\n",
    "true_slope = 2\n",
    "true_intercept = 5\n",
    "Y = true_slope * X + true_intercept + np.random.normal(0, 2, 100)  # Dependent variable with noise\n",
    "\n",
    "# Add constant for the intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# fit the model\n",
    "model = sm.OLS(Y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# Print results\n",
    "print(results.summary())\n",
    "\n",
    "#Plot the data and regression line\n",
    "x_values = np.linspace(0,10, 100)\n",
    "x_values_const= sm.add_constant(x_values)\n",
    "predicted_values= results.predict(x_values_const)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:, 1], Y, label=\"Data Points\") #X is array with constant, plot the second column\n",
    "plt.plot(x_values, predicted_values, label=\"Regression Line\", color='red')\n",
    "plt.xlabel(\"Independent Variable\")\n",
    "plt.ylabel(\"Dependent Variable\")\n",
    "plt.title(\"Simple Linear Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Multiple Linear Regression ---\n",
    "print(\"\\nMultiple Linear Regression Example:\")\n",
    "\n",
    "# Generate multiple features\n",
    "X = np.random.rand(100, 2) * 10\n",
    "true_coef = [1.5, -0.5] # coefficients for multiple variables\n",
    "true_intercept = 3\n",
    "y_multiple = np.dot(X, true_coef) + true_intercept + np.random.normal(0, 2, 100) # Dependent\n",
    "\n",
    "# Add constant for the intercept\n",
    "X = sm.add_constant(X)\n",
    "model_multiple = sm.OLS(y_multiple, X)\n",
    "results_multiple = model_multiple.fit()\n",
    "print(results_multiple.summary())\n",
    "\n",
    "#Create new model using sklearn to predict data\n",
    "model_sklearn = LinearRegression()\n",
    "model_sklearn.fit(X[:, 1:], y_multiple)\n",
    "\n",
    "# Predict with new model\n",
    "x_plot_2d = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "x_plot_2d_expanded = np.concatenate((x_plot_2d, x_plot_2d), axis=1)\n",
    "y_plot_multiple = model_sklearn.predict(x_plot_2d_expanded)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(X[:, 1], X[:, 2], y_multiple, label=\"Data Points\")\n",
    "ax.plot3D(x_plot_2d.squeeze(), x_plot_2d.squeeze(), y_plot_multiple, label='Regression Plane', color='red')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_zlabel('Target Variable')\n",
    "ax.set_title(\"Multiple Linear Regression\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"Both simple and multiple regression models can be fitted.\")\n",
    "print(\"The summary table shows the coefficents of the fitted models, and the R-squared value (measures model fit).\")\n",
    "print(\"The multiple linear regression plot shows how the plane fits the 3D data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
