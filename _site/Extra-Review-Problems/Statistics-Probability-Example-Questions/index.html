<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Statistics &amp; Probability for Meta Data Science Interviews</title>
    <meta name="description" content="A handbook for preparing for analytical/data-science interviews">

    <!-- MathJax -->
    <script async src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css">

    <!-- Site CSS -->
    <link rel="stylesheet" href="/assets/css/style.css">
  </head>
  <body>
    <aside class="sidebar">
  <h2>Contents</h2>
  <ul class="nav-list">
    <li><a href="/introduction/" class="">I. Introduction</a>
      <ul>
        <li><a href="/introduction/#welcome">1. Welcome and Purpose of this Handbook</a></li>
        <li><a href="/introduction/#meta-data-science-role">2. What to Expect: The Meta Data Science Role</a></li>
        <li><a href="/introduction/#interview-process">3. Navigating the Meta Interview Process</a></li>
        <li><a href="/introduction/#handbook-usage">4. How to Use This Handbook</a></li>
      </ul>
    </li>
    <li><a href="/foundational_knowledge/1/">II. Foundational Knowledge & Skills</a></li>
    <li><a href="/interview_preparation/technical_skills/">III. Interview Preparation</a></li>
    <li><a href="/meta_specificity/">IV. Meta Specificity</a></li>
    <li><a href="/resources_practice/">V. Resources and Practice</a></li>
    <li><a href="/conclusion/">VI. Conclusion</a></li>
    <li><a href="/appendix/">Appendix</a></li>
  </ul>
</aside>


    <main class="main-content">
      <h1 id="statistics--probability-for-meta-data-science-interviews">Statistics &amp; Probability for Meta Data Science Interviews</h1>

<p>This page provides a detailed review of statistics and probability concepts relevant to Meta Data Science (Analytical) interviews. It is intended as a supplement to the main Meta Data Science Interview Preparation Handbook.</p>

<hr />

<div style="page-break-after: always;"></div>

<h2 id="detailed-concepts-and-practice-questions">Detailed Concepts and Practice Questions</h2>

<h3 id="probability-and-combinations">Probability and Combinations</h3>

<p><strong>Question 1: On Instagram, the probability of a user watching a story to completion is 0.8. If a user posts a sequence of 4 stories, what is the probability that a viewer will watch all 4 stories? What about at least 2 stories?</strong></p>

<p><strong>Key Concept(s):</strong> Independent Events, Binomial Probability</p>

<p><strong>Explanation:</strong> This question tests understanding of independent events and binomial probability (for the “at least 2” part).</p>

<ul>
  <li><strong>Watching all 4:</strong> Since each story view is independent, we multiply the probabilities: (0.8 * 0.8 * 0.8 * 0.8 = 0.8^4 = 0.4096) or 40.96%</li>
  <li><strong>Watching at least 2:</strong> This means watching 2, 3, or 4 stories. It’s easier to calculate the complement (0 or 1 story) and subtract from 1.
    <ul>
      <li>0 stories: (0.2^4 = 0.0016)</li>
      <li>1 story: (\binom{4}{1} * 0.8 * 0.2^3 = 4 * 0.8 * 0.008 = 0.0256)</li>
      <li>P(at least 2) = (1 - (0.0016 + 0.0256) = 1 - 0.0272 = 0.9728) or 97.28%</li>
    </ul>
  </li>
</ul>

<p><strong>Answer:</strong> The probability of watching all 4 stories is 40.96%. The probability of watching at least 2 is 97.28%.</p>

<div style="page-break-after: always;"></div>

<h3 id="hypothesis-testing">Hypothesis Testing</h3>

<p><strong>Question 2: What is the difference between Type I and Type II errors in hypothesis testing?</strong></p>

<p><strong>Key Concept(s):</strong> Type I Error, Type II Error, Null Hypothesis, Alternative Hypothesis</p>

<p><strong>Explanation:</strong> This is a fundamental concept in hypothesis testing.</p>

<ul>
  <li><strong>Type I Error (False Positive):</strong> Rejecting the null hypothesis when it is actually true. This is often represented by (\alpha) (alpha), the significance level. Example: Concluding that a new feature improves conversion rates when it actually has no effect.</li>
  <li><strong>Type II Error (False Negative):</strong> Failing to reject the null hypothesis when it is actually false. This is often represented by (\beta) (beta). Example: Concluding that a new feature has no effect on conversion rates when it actually does improve them.</li>
</ul>

<p><strong>Answer:</strong> A Type I error is a false positive (incorrectly rejecting a true null hypothesis), while a Type II error is a false negative (incorrectly failing to reject a false null hypothesis).</p>

<div style="page-break-after: always;"></div>

<h3 id="probability">Probability</h3>

<p><strong>Question 3: Say you roll a die three times. What is the probability of getting two sixes in a row?</strong></p>

<p><strong>Key Concept(s):</strong> Independent Events, Probability</p>

<p><strong>Explanation:</strong> This involves calculating probabilities of sequential events.</p>

<p>There are (6^3 = 216) possible outcomes when rolling a die three times. The sequences with two sixes in a row are:</p>

<ul>
  <li>6, 6, 1</li>
  <li>6, 6, 2</li>
  <li>6, 6, 3</li>
  <li>6, 6, 4</li>
  <li>6, 6, 5</li>
  <li>6, 6, 6</li>
  <li>1, 6, 6</li>
  <li>2, 6, 6</li>
  <li>3, 6, 6</li>
  <li>4, 6, 6</li>
  <li>5, 6, 6</li>
</ul>

<p>There are 11 such outcomes.</p>

<p><strong>Answer:</strong> The probability is 11/216, or approximately 5.09%.</p>

<div style="page-break-after: always;"></div>

<h3 id="statistical-inference">Statistical Inference</h3>
<p><strong>Question 4: Can you explain what a p-value and confidence interval are, but in layman’s terms?</strong></p>

<p><strong>Key Concept(s):</strong> p-value, Confidence Interval, Statistical Significance</p>

<p><strong>Explanation:</strong> These are key concepts for interpreting statistical results.</p>

<ul>
  <li><strong>p-value:</strong> The probability of observing the results (or more extreme results) of your experiment if there were actually no real effect (if the null hypothesis were true). A small p-value (typically below 0.05) suggests strong evidence against the null hypothesis. Imagine you’re testing a new drug. If the p-value is 0.03, it means there’s only a 3% chance you’d see the observed improvement in patients if the drug was actually useless.</li>
  <li><strong>Confidence Interval:</strong> A range of values that is likely to contain the true population parameter (like the average user session time or conversion rate). A 95% confidence interval means that if you were to repeat the experiment many times, 95% of the calculated intervals would contain the true population value. For example, a 95% confidence interval for average session time of (10-12 seconds) means that we are 95% sure that the true average session time lies between 10 and 12 seconds.</li>
</ul>

<p><strong>Answer:</strong> A p-value tells you how surprising your results are if there’s no real effect. A confidence interval gives you a plausible range for the true value you’re trying to estimate.</p>

<div style="page-break-after: always;"></div>

<h3 id="statistical-relationships">Statistical Relationships</h3>

<p><strong>Question 5: Explain the concept of covariance and correlation. How are they different, and what do they measure?</strong></p>

<p><strong>Key Concept(s):</strong> Covariance, Correlation</p>

<p><strong>Explanation:</strong> These measures describe the relationship between two variables.</p>

<ul>
  <li><strong>Covariance:</strong> Measures how two variables change together. A positive covariance means they tend to increase or decrease together, while a negative covariance means they tend to move in opposite directions. However, the magnitude of covariance is difficult to interpret because it’s dependent on the scales of the variables.</li>
  <li><strong>Correlation:</strong> A standardized measure of the linear relationship between two variables. It ranges from -1 to +1. -1 indicates a perfect negative correlation, +1 indicates a perfect positive correlation, and 0 indicates no linear correlation. Correlation is easier to interpret than covariance because it’s on a fixed scale.</li>
</ul>

<p><strong>Answer:</strong> Covariance measures the direction of a linear relationship, while correlation measures both the direction and strength of the linear relationship, making it easier to compare relationships between different pairs of variables.</p>

<div style="page-break-after: always;"></div>

<h3 id="bayes-theorem-applied">Bayes’ Theorem (Applied)</h3>

<p><strong>Question 6: A Facebook Ads analyst is investigating the effectiveness of a new ad targeting algorithm. As a general baseline, they know that 1% of all users who see an ad convert (make a purchase). The new algorithm correctly identifies 80% of users who will convert for an ad. The algorithm also incorrectly flags 10% of non-converting users as likely to convert. Given that the algorithm has flagged a user as likely to convert, what is the probability that this user will actually convert?</strong></p>

<p><strong>Key Concept(s):</strong> Bayes’ Theorem, Conditional Probability</p>

<p><strong>Explanation:</strong> This is a classic application of Bayes’ Theorem.</p>

<ul>
  <li>(P(\text{Convert}) = 0.01) (Prior probability of conversion)</li>
  <li>( P(\text{Not Convert}) = 0.99 )</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>(P(\text{Flagged}</td>
          <td>\text{Convert}) = 0.80) (True Positive Rate)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>(P(\text{Flagged}</td>
          <td>\text{Not Convert}) = 0.10) (False Positive Rate)</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<table>
  <tbody>
    <tr>
      <td>We want to find (P(\text{Convert}</td>
      <td>\text{Flagged})). Using Bayes’ Theorem:</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>(P(\text{Convert}</td>
      <td>\text{Flagged}) = \frac{P(\text{Flagged}</td>
      <td>\text{Convert}) * P(\text{Convert})}{P(\text{Flagged})})</td>
    </tr>
  </tbody>
</table>

<p>First, we need to calculate (P(\text{Flagged})):</p>

<p>(P(\text{Flagged}) = P(\text{Flagged} | \text{Convert}) * P(\text{Convert}) + P(\text{Flagged} | \text{Not Convert}) * P(\text{Not Convert}))
(P(\text{Flagged}) = (0.80 * 0.01) + (0.10 * 0.99) = 0.008 + 0.099 = 0.107)</p>

<table>
  <tbody>
    <tr>
      <td>Now we can calculate (P(\text{Convert}</td>
      <td>\text{Flagged})):</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>(P(\text{Convert}</td>
      <td>\text{Flagged}) = (0.80 * 0.01) / 0.107 = 0.008 / 0.107 \approx 0.0748) or 7.48%</td>
    </tr>
  </tbody>
</table>

<p><strong>Answer:</strong> Given that the algorithm has flagged a user as likely to convert, there is approximately a 7.48% chance that the user will actually convert. This highlights that even with a good algorithm, if the base conversion rate is very low, the positive predictions will still have a relatively high false positive rate.</p>

<div style="page-break-after: always;"></div>

<h3 id="hypothesis-testing-ab-testing">Hypothesis Testing (A/B Testing)</h3>

<p><strong>Question 7: You’re analyzing user engagement with a new feature. You observe that users who interact with the feature spend significantly more time on the platform. How would you determine if this increase in time spent is statistically significant?</strong></p>

<p><strong>Key Concept(s):</strong> Hypothesis Testing, A/B Testing, Statistical Significance, Z-test, Mann-Whitney U test</p>

<p><strong>Answer:</strong></p>

<ol>
  <li>
    <p><strong>Define Hypotheses:</strong></p>

    <ul>
      <li>
        <p>Null Hypothesis (H0): There is no significant difference in the proportion (e.g., CTR) between the control group (A) and the treatment group (B). Mathematically:</p>

        <p>( p_A = p_B )  or equivalently ( p_A - p_B = 0 )</p>

        <p>Where:</p>

        <ul>
          <li>( p_A ) is the proportion (e.g., CTR) of the control group.</li>
          <li>( p_B ) is the proportion of the treatment group.</li>
        </ul>
      </li>
      <li>
        <p>Alternative Hypothesis (H1): There is a significant difference in the proportion between the two groups. This is a two-tailed test (we’re not specifying the direction). Mathematically:</p>

        <p>( p_A \ne p_B ) or equivalently ( p_A - p_B \ne 0 )</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Choose a Test:</strong> A two-sample Z-test for proportions is appropriate when we are dealing with proportions (like click-through rates, conversion rates) and the sample sizes are large enough to invoke the Central Limit Theorem.</p>
  </li>
  <li>
    <p><strong>Calculate the Test Statistic (Z-score) and p-value:</strong></p>

    <p>To calculate the Z-score and p-value, we follow these steps:</p>

    <ul>
      <li><strong>Collect Data:</strong> Let:
        <ul>
          <li>( x_A ) = Number of successes (e.g., clicks) in group A</li>
          <li>( n_A ) = Total number of trials (e.g., impressions) in group A</li>
          <li>( x_B ) = Number of successes in group B</li>
          <li>( n_B ) = Total number of trials in group B</li>
        </ul>
      </li>
      <li><strong>Calculate Observed Proportions:</strong>
        <ul>
          <li>(\hat{p}_A = \frac{x_A}{n_A})</li>
          <li>(\hat{p}_B = \frac{x_B}{n_B})</li>
        </ul>
      </li>
      <li><strong>Calculate the Pooled Proportion ((\hat{p})):</strong>
        <ul>
          <li>(\hat{p} = \frac{x_A + x_B}{n_A + n_B})</li>
        </ul>
      </li>
      <li><strong>Calculate the Standard Error of the Difference (SE):</strong>
        <ul>
          <li>(SE = \sqrt{\hat{p}(1 - \hat{p}) \left(\frac{1}{n_A} + \frac{1}{n_B}\right)})</li>
        </ul>
      </li>
      <li><strong>Calculate the Z-score:</strong>
        <ul>
          <li>(Z = \frac{\hat{p}_B - \hat{p}_A}{SE})</li>
        </ul>
      </li>
      <li><strong>Calculate the p-value:</strong> For a two-tailed test:
        <ul>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>(p\text{-value} = 2 \times P(Z &gt;</td>
                  <td>Z</td>
                  <td>))</td>
                </tr>
              </tbody>
            </table>
          </li>
        </ul>

        <table>
          <tbody>
            <tr>
              <td>Where (P(Z &gt;</td>
              <td>Z</td>
              <td>)) is found using a Z-table or statistical software.</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Interpret the Results:</strong></p>

    <ul>
      <li>Compare the p-value to the chosen significance level ((\alpha)), typically 0.05.</li>
      <li><strong>If (p\text{-value} \le \alpha):</strong> Reject the null hypothesis (H0). This indicates that there is statistically significant evidence of a difference in proportions between the two groups.</li>
      <li><strong>If (p\text{-value} &gt; \alpha):</strong> Fail to reject the null hypothesis (H0). This means there is not enough evidence to conclude a statistically significant difference.</li>
    </ul>

    <p>In addition to statistical significance (determined by the p-value), it’s crucial to consider the <em>effect size</em>. The effect size quantifies the magnitude of the observed difference. Even if a difference is statistically significant, it might be too small to be practically meaningful. For proportions, common effect size measures include the absolute difference ((\hat{p}_B - \hat{p}_A)) or relative change.</p>
  </li>
</ol>

<div style="page-break-after: always;"></div>

<p><strong>Question 8: Explain the difference between a one-tailed and a two-tailed hypothesis test. When would you use each?</strong></p>

<p><strong>Key Concept(s):</strong> One-tailed Test, Two-tailed Test, Hypothesis Testing</p>

<p><strong>Explanation:</strong> This is about the directionality of the hypothesis.</p>

<ul>
  <li><strong>One-tailed Test:</strong> Tests for a difference in <em>one</em> direction. For example, testing if a new feature <em>increases</em> conversion rates. The alternative hypothesis specifies a direction (e.g.,  (\mu &gt; \mu_0) or (\mu &lt; \mu_0)).</li>
  <li><strong>Two-tailed Test:</strong> Tests for a difference in <em>either</em> direction. For example, testing if a new feature has <em>any</em> effect (positive or negative) on conversion rates. The alternative hypothesis simply states that there is a difference (e.g., (\mu \ne \mu_0)).</li>
</ul>

<p><strong>Answer:</strong> Use a one-tailed test when you have a specific directional hypothesis (you only care about an increase or a decrease). Use a two-tailed test when you’re interested in any difference, regardless of direction.</p>

<div style="page-break-after: always;"></div>

<p><strong>Question 9: You have data on user sign-ups for your platform. You notice a large spike in sign-ups during a particular week. How would you investigate the cause of this spike?</strong></p>

<p><strong>Key Concept(s):</strong> Data Analysis, Segmentation, Time Series Analysis, Correlation Analysis</p>

<p><strong>Explanation:</strong> This tests analytical thinking and applying statistical concepts to a real-world problem.</p>

<p><strong>Answer:</strong></p>

<ol>
  <li><strong>Data Validation:</strong> Ensure the data is accurate and not due to a tracking error.</li>
  <li><strong>External Factors:</strong> Check for any external events that might have driven the spike (e.g., marketing campaigns, media coverage, competitor issues, holidays, viral trends).</li>
  <li><strong>Segmentation:</strong> Analyze sign-ups by different user segments (e.g., demographics, acquisition channel, device type, location) to see if the spike is concentrated in any particular group.</li>
  <li><strong>A/B Testing (if applicable):</strong> If any changes were made to the sign-up process during that week (e.g., new landing page, simplified form), analyze the A/B test results to see if they contributed to the spike.</li>
  <li><strong>Time Series Analysis:</strong> Look for patterns in sign-up data over time (e.g., seasonality, weekly trends) to see if the spike is unusual or part of a recurring trend.</li>
  <li><strong>Correlation Analysis:</strong> Examine correlations between sign-ups and other metrics (e.g., website traffic, social media mentions, search volume, app store rankings) to identify potential contributing factors.</li>
</ol>

<div style="page-break-after: always;"></div>

<p><strong>Question 10: What are some methods for dealing with imbalanced datasets, and why are they important?</strong></p>

<p><strong>Key Concept(s):</strong> Imbalanced Datasets, Oversampling, Undersampling, SMOTE, Cost-Sensitive Learning</p>

<p><strong>Explanation:</strong> This is important in machine learning and data analysis when one class is significantly more prevalent than another.</p>

<p><strong>Answer:</strong> Imbalanced datasets can bias models towards the majority class. Methods to address this include:</p>

<ul>
  <li><strong>Oversampling:</strong> Duplicating instances from the minority class.</li>
  <li><strong>Undersampling:</strong> Removing instances from the majority class.</li>
  <li><strong>SMOTE (Synthetic Minority Over-sampling Technique):</strong> Creating synthetic instances of the minority class by interpolating between existing minority class instances.</li>
  <li><strong>Cost-sensitive learning:</strong> Assigning higher misclassification costs to the minority class during model training.</li>
  <li><strong>Collecting more data:</strong> If possible, collecting more data for the minority class can help balance the dataset.</li>
</ul>

<p>These methods are important because they help models learn to accurately predict the minority class, which is often the class of interest (e.g., fraud detection, churn prediction).</p>

<div style="page-break-after: always;"></div>

<p><strong>Question 11: Explain the concept of statistical power and its importance in experiment design.</strong></p>

<p><strong>Key Concept(s):</strong> Statistical Power, Type II Error, Sample Size, Effect Size, Significance Level</p>

<p><strong>Explanation:</strong> Relates to the ability of a test to find an effect if one exists.</p>

<p><strong>Answer:</strong> Statistical power is the probability of correctly rejecting the null hypothesis when it is false (i.e., finding a real effect). It’s typically set at 80% (0.8). Low power means the experiment has a low chance of detecting a true effect, leading to potential Type II errors (false negatives). Factors affecting power include:</p>

<ul>
  <li><strong>Sample Size:</strong> Larger sample sizes increase power.</li>
  <li><strong>Effect Size:</strong> Larger effects are easier to detect (higher power).</li>
  <li><strong>Significance Level (alpha):</strong> Lower alpha (e.g., 0.01 instead of 0.05) decreases power.</li>
  <li><strong>Variability (Standard Deviation):</strong> Lower variability in the data increases power.</li>
</ul>

<p>Power analysis is crucial in experiment design to determine the appropriate sample size needed to achieve the desired power. It helps avoid underpowered studies that may fail to detect real effects, wasting resources and time.</p>

<div style="page-break-after: always;"></div>

<h3 id="harder-practice-questions-social-network-focus">Harder Practice Questions (Social Network Focus)</h3>

<p><strong>Question 18: Confidence Intervals - Click-Through Rates:</strong> You’re analyzing the click-through rate (CTR) of a new ad campaign. You observe a CTR of 2% from a sample of 10,000 impressions. Calculate a 95% confidence interval for the true CTR. How would a larger sample size (e.g. 100,000 impressions) impact the width of the confidence interval?**</p>

<p><strong>Key Concept(s):</strong> Confidence Intervals, Proportions, Sample Size</p>

<p><strong>Explanation:</strong> This tests the application and interpretation of confidence intervals.</p>

<p><strong>Answer:</strong></p>

<ol>
  <li><strong>Calculate the Standard Error (n=10,000):</strong> (SE = \sqrt{\frac{p(1-p)}{n}} = \sqrt{\frac{(0.02 * 0.98)}{10000}} \approx 0.0014)</li>
  <li><strong>Find the Z-score:</strong> For a 95% confidence interval, the z-score is approximately 1.96.</li>
  <li><strong>Calculate the Confidence Interval (n=10,000):</strong> (CI = p \pm z * SE = 0.02 \pm 1.96 * 0.0014 \approx (0.0173, 0.0227)) or (1.73%, 2.27%)</li>
</ol>

<p>With a larger sample size (100,000 impressions):</p>

<ol>
  <li><strong>Calculate the Standard Error (n=100,000):</strong> (SE = \sqrt{\frac{(0.02 * 0.98)}{100000}} \approx 0.00044)</li>
  <li><strong>Calculate the Confidence Interval (n=100,000):</strong> (CI = 0.02 \pm 1.96 * 0.00044 \approx (0.0191, 0.0209)) or (1.91%, 2.09%)</li>
</ol>

<p><strong>Impact of Sample Size:</strong> As you can see, the confidence interval is much narrower with the larger sample size. This is because the standard error decreases as the sample size increases, leading to a more precise estimate of the true CTR.</p>

<div style="page-break-after: always;"></div>

<p><strong>Question 19: Combining Concepts - Outliers, Central Limit Theorem, and A/B Testing:</strong> You are A/B testing a new algorithm for ranking posts in a user’s feed. You are measuring the average time spent per session. You notice some users have extremely long session durations (potential outliers).</p>

<ol>
  <li>How would you handle these outliers before conducting your statistical test?</li>
  <li>How does the Central Limit Theorem help you in this scenario, given that session durations are likely right-skewed?</li>
  <li>What statistical test would you use to compare the average time spent between the control and treatment groups?</li>
</ol>

<p><strong>Key Concept(s):</strong> Outliers, Central Limit Theorem, A/B Testing, t-test, Mann-Whitney U test</p>

<p><strong>Explanation:</strong> This question combines several important concepts.</p>

<p><strong>Answer:</strong></p>

<ol>
  <li><strong>Handling Outliers:</strong>
    <ul>
      <li><strong>Investigate:</strong> Determine if the long session durations are due to errors (e.g., app crashes, background activity) or genuine user behavior (e.g., users leaving the app open).</li>
      <li><strong>Transformation:</strong> Consider a log transformation of the session duration data to reduce the influence of outliers and potentially normalize the distribution. This is often a good approach for right-skewed data like session durations.</li>
      <li><strong>Winsorizing/Trimming:</strong> Replace extreme values with less extreme values (Winsorizing) or remove them entirely (Trimming). This should be done cautiously and with a clear justification.</li>
    </ul>
  </li>
  <li><strong>Central Limit Theorem:</strong> The Central Limit Theorem states that the distribution of the <em>sample means</em> of session duration will be approximately normal, even if individual session durations are right-skewed, <em>provided the sample size is large enough</em>. This is crucial because it allows you to use statistical tests that assume normality (like t-tests or z-tests) to make inferences about the <em>population mean</em> session duration, even though the original data is not normally distributed.</li>
  <li><strong>Statistical Test:</strong>
    <ul>
      <li>If the data is approximately normally distributed <em>after handling outliers/transformations</em> and the sample sizes are reasonably large, a two-sample t-test is appropriate.</li>
      <li>If the data remains skewed even after transformation or if the sample size is small, the Mann-Whitney U test (a non-parametric test) is a better choice as it does not assume normality.</li>
    </ul>
  </li>
</ol>

<div style="page-break-after: always;"></div>

<p><strong>Question 20: Joint Probability and Conditional Probability - User Demographics and Feature Usage:</strong> You have the following data about user demographics and usage of a new feature:</p>

<table>
  <thead>
    <tr>
      <th>Demographic</th>
      <th>Used Feature</th>
      <th>Did Not Use Feature</th>
      <th>Total</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Age 18-24</td>
      <td>300</td>
      <td>200</td>
      <td>500</td>
    </tr>
    <tr>
      <td>Age 25-34</td>
      <td>400</td>
      <td>100</td>
      <td>500</td>
    </tr>
    <tr>
      <td>Total</td>
      <td>700</td>
      <td>300</td>
      <td>1000</td>
    </tr>
  </tbody>
</table>

<ol>
  <li>What is the probability that a randomly selected user is aged 18-24 <em>and</em> used the feature?</li>
  <li>What is the probability that a user used the feature <em>given</em> they are aged 25-34?</li>
  <li>Are age demographic and feature usage independent events?</li>
</ol>

<p><strong>Key Concept(s):</strong> Joint Probability, Conditional Probability, Independence</p>

<p><strong>Explanation:</strong> This tests understanding of joint and conditional probabilities and how to assess independence.</p>

<p><strong>Answer:</strong></p>

<ol>
  <li><strong>P(Age 18-24 and Used Feature):</strong> 300/1000 = 0.3</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>**P(Used Feature</td>
          <td>Age 25-34):** 400/500 = 0.8</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><strong>Independence:</strong>
    <ul>
      <li>P(Age 18-24) = 500/1000 = 0.5</li>
      <li>P(Used Feature) = 700/1000 = 0.7</li>
      <li>If independent, P(Age 18-24 and Used Feature) should equal P(Age 18-24) * P(Used Feature) = 0.5 * 0.7 = 0.35</li>
      <li>Since 0.3 ≠ 0.35, the events are <em>not</em> independent.</li>
    </ul>
  </li>
</ol>

<div style="page-break-after: always;"></div>

<p><strong>Question 21: Confidence Intervals and A/B Testing for Engagement:</strong> You run an A/B test on a new notification system designed to increase user engagement. You measure the average number of sessions per week. The control group (A) has a mean of 5.2 sessions with a standard deviation of 1.5, and the treatment group (B) has a mean of 5.5 sessions with a standard deviation of 1.6. Both groups have a sample size of 5000 users.</p>

<ol>
  <li>Calculate a 95% confidence interval for the difference in means between the two groups.</li>
  <li>Based on the confidence interval, can you conclude that the new notification system significantly increased user engagement?</li>
</ol>

<p><strong>Key Concept(s):</strong> Confidence Intervals, A/B Testing, Difference of Means, Standard Error</p>

<p><strong>Explanation:</strong> This tests understanding of confidence intervals in the context of A/B testing.</p>

<p><strong>Answer:</strong></p>

<ol>
  <li>
    <p><strong>Calculate the Standard Error of the Difference (SE):</strong></p>

    <p>The standard error of the difference between two means is calculated as:</p>

    <p>$SE = \sqrt{\frac{SD_A^2}{n_A} + \frac{SD_B^2}{n_B}}$</p>

    <p>Where:</p>

    <ul>
      <li>$SD_A$ is the standard deviation of group A.</li>
      <li>$SD_B$ is the standard deviation of group B.</li>
      <li>$n_A$ is the sample size of group A.</li>
      <li>$n_B$ is the sample size of group B.</li>
    </ul>

    <p>Plugging in the given values:</p>

    <p>$SE = \sqrt{\frac{1.5^2}{5000} + \frac{1.6^2}{5000}} = \sqrt{\frac{2.25}{5000} + \frac{2.56}{5000}} = \sqrt{0.00045 + 0.000512} = \sqrt{0.000962} \approx 0.031$</p>
  </li>
  <li>
    <p><strong>Find the Z-score:</strong></p>

    <p>For a 95% confidence interval, the Z-score is approximately 1.96. This value corresponds to the point on the standard normal distribution where 95% of the data falls within $\pm$ 1.96 standard deviations of the mean.</p>
  </li>
  <li>
    <p><strong>Calculate the Confidence Interval for the Difference in Means:</strong></p>

    <p>The confidence interval for the difference in means is calculated as:</p>

    <p>$CI = (\bar{x}_B - \bar{x}_A) \pm z \times SE$</p>

    <p>Where:</p>

    <ul>
      <li>$\bar{x}_A$ is the mean of group A.</li>
      <li>$\bar{x}_B$ is the mean of group B.</li>
      <li>$z$ is the Z-score corresponding to the desired confidence level.</li>
    </ul>

    <p>Plugging in the values:</p>

    <p>$CI = (5.5 - 5.2) \pm 1.96 \times 0.031 = 0.3 \pm 0.061 \approx (0.239, 0.361)$</p>
  </li>
  <li>
    <p><strong>Conclusion:</strong></p>

    <p>Since the 95% confidence interval (0.239, 0.361) does <em>not</em> contain zero, we can conclude with 95% confidence that there is a statistically significant difference in the average number of sessions per week between the two groups. Because the entire interval is positive, we can confidently state that the new notification system <em>significantly increased</em> user engagement, as measured by sessions per week. The average number of sessions per week in group B is likely to be between 0.239 and 0.361 sessions greater than in group A.</p>
  </li>
</ol>

<div style="page-break-after: always;"></div>

<h1 id="additional-statistics--probability-practice-questions-for-meta-data-science-interviews">Additional Statistics &amp; Probability Practice Questions for Meta Data Science Interviews</h1>

<p>These questions build upon the previous set, offering more practice and deeper insights into the required concepts for a Meta Data Science (Analytical) interview.</p>

<hr />
<div style="page-break-after: always;"></div>

<h3 id="probability-and-combinations-1">Probability and Combinations</h3>

<p><strong>Question 1: A social media platform notices that 70% of users like at least one post per day. If you randomly select 5 users, what is the probability that exactly 3 of them like at least one post daily?</strong></p>

<p><strong>Key Concept(s):</strong> Binomial Probability</p>

<p><strong>Explanation:</strong> This question tests understanding of the binomial probability distribution.</p>

<ul>
  <li><strong>Setup</strong>: (n=5), (p=0.7), and we want the probability of exactly 3 successes ((k=3)). We will need to use the formula
(P(X=k) = \binom{n}{k} * p^k * (1-p)^{n-k})
(P(X=3) = \binom{5}{3} * 0.7^3 * (1-0.7)^{5-3} =  \frac{5!}{3!2!} * 0.7^3 * 0.3^2 = 10* 0.343 * 0.09= 0.3087 )</li>
</ul>

<p><strong>Answer:</strong> The probability that exactly 3 out of 5 users like at least one post per day is approximately 0.3087 or 30.87%.</p>

<div style="page-break-after: always;"></div>

<p><strong>Question 2: You have a bag containing 8 red balls and 4 blue balls. If you draw two balls without replacement, what is the probability that both balls are red?</strong></p>

<p><strong>Key Concept(s):</strong> Conditional Probability, Dependent Events</p>

<p><strong>Explanation:</strong> This problem tests understanding of probability without replacement.</p>

<ul>
  <li><strong>1st ball is red</strong>: There are 12 total balls so probability is (\frac{8}{12})</li>
  <li><strong>2nd ball is red</strong>: Given the first ball is red, then there are 7 red balls and 11 balls total left so probability is (\frac{7}{11})</li>
  <li>The probabilty of the two independent steps is (\frac{8}{12} * \frac{7}{11} = \frac{56}{132} = 0.424).</li>
</ul>

<p><strong>Answer:</strong> The probability that both balls drawn are red is approximately (\frac{56}{132} ) which equals approximately 0.424.</p>

<div style="page-break-after: always;"></div>

<h3 id="hypothesis-testing-1">Hypothesis Testing</h3>

<p><strong>Question 3: Define what a Null Hypothesis is and explain in the context of a A/B test example.</strong></p>

<p><strong>Key Concept(s):</strong> Null Hypothesis, A/B Testing</p>

<p><strong>Explanation:</strong> Tests understanding of the fundamental concept of the Null Hypothesis</p>

<ul>
  <li>The null hypothesis in statistical testing, such as during A/B testing, is the statement that there is no significant difference or effect between different scenarios, variables, or groups that are being studied. It essentially asserts that any changes observed in our testing or observation are not because of any real phenomena being studied but are due to random chance. An A/B testing context is if we want to understand the effectiveness of a redesign to the UI interface and so an appropriate Null hypothesis is “There is no statistically significant change between interface A and Interface B when looking at conversion rates”.</li>
</ul>

<p><strong>Answer:</strong> The Null Hypothesis states there is no effect and it is what you set out to disprove.</p>

<div style="page-break-after: always;"></div>

<p><strong>Question 4: In hypothesis testing, what is the relationship between the p-value and the significance level? Explain with example scenarios when you would chose to use differing levels of signifcance.</strong></p>

<p><strong>Key Concept(s):</strong> p-value, Significance Level</p>

<p><strong>Explanation:</strong> Tests knowledge of statistical concepts and implications</p>

<ul>
  <li>The p-value is compared against the chosen significance level (\alpha) (alpha) to determine if the results are statistically significant and if the null hypothesis is to be rejected.</li>
  <li>The significance level ((\alpha)), is typically set at a chosen threshold (often at 0.05), to establish how rare your observation of differences under a Null hypothesis scenario has to be in order to deem our findings statistically signficant.</li>
  <li>A p-value smaller than  (\alpha) results in rejecting the Null Hypothesis. If the (p&gt;\alpha), then fail to reject.</li>
  <li>The threshold that is chosen, will relate to your real world needs and so (\alpha=0.05)  is not always the right approach. If you were working on high-risk scenarios (e.g., drug trials), a lower (\alpha) (such as (\alpha)= 0.01 or 0.001) is required. When the risk of falsely concluding that a change makes no impact ( when it in fact might exist) has a higher cost, then you would increase (\alpha) above (0.05), like 0.10 for less crucial experiments</li>
</ul>

<p><strong>Answer:</strong>  The p-value is directly compared with the pre-selected  significance level ((\alpha)) to see if findings are statistically significant to reject the null hypothesis, the choice of (\alpha) needs to reflect the cost of Type 1 errors for a given research context</p>

<div style="page-break-after: always;"></div>

<h3 id="statistical-inference-1">Statistical Inference</h3>

<p><strong>Question 5: Describe what a histogram visually represents and discuss how it is different from a box plot, in practical terms.</strong></p>

<p><strong>Key Concept(s):</strong> Data Visualization, Histogram, Box Plot</p>

<p><strong>Explanation:</strong> Tests understanding of common methods to present data.</p>

<ul>
  <li>
    <p><strong>Histogram:</strong> A histogram displays the distribution of numerical data, grouping it into bins, which the counts then become represented by vertical bars. They highlight how frequent numerical data values or a specified bin occur in the dataset. Histograms show the shape of the distribution, such as being normal or skewed.</p>
  </li>
  <li>
    <p><strong>Box Plot</strong>:  Box plots, use median and quartiles to demonstrate key percentiles. Box Plots are ideal for understanding measures like inter quartile range and outlier values of your numerical data. They display data by dividing the data into quarters and so less insightful to understand distributional patterns. They show general distribution spread, whereas histograms can visually represent a higher resolution into those distributions and how their underlying nature. Box plots can quickly compare different group of data on key metrics like median and percentiles, histograms generally only display one series of data at one time.</p>
  </li>
</ul>

<p><strong>Answer:</strong> Histograms represent the distribution via frequencies for values of your data, whilst boxplots present key statistical points that show distribution of your dataset and outlier metrics. The use depends on the task at hand and how you wish to represent the core information of a dataset..</p>

<div style="page-break-after: always;"></div>

<p><strong>Question 6:  You want to know the average daily time a user spends on your application. Explain when you would use the sample mean vs when to use the median.</strong></p>

<p><strong>Key Concept(s):</strong> Measures of Central Tendency, Sample Mean, Median</p>

<p><strong>Explanation:</strong> Tests awareness of when it is more apprpriate to use sample mean or median.</p>

<ul>
  <li>The <strong>sample mean</strong>, or average,  is appropriate when the data are reasonably symmetrical or roughly normal and there is little to no outlier data. This can also work if large enough sample is observed to make any distortions small enough due to outlier issues.</li>
  <li>The <strong>median</strong> on the other hand is more appropriate if your data shows skewed distributions, due to outliers as they wont influence this measure so readily, and often results in better reflection of central tendency in many use case contexts.</li>
  <li>The <strong>Median</strong> could be very insightful, if you wanted to measure something where you felt most common users don’t exhibit an effect of your population parameter (time in application), with outlier values being an unwanted distortion for such analysis. For example If a small subsegment uses the platform for a vast length of time vs everyone else.</li>
  <li>Often use mean as one method to check if the distribution has outlier behaviour where the median would likely also be evaluated if means appears far away, when that can indicate skew that might influence average calculations for certain measures.
 <strong>Answer:</strong> Sample means are appropriate for relatively normal distributions, but you need to observe distribution and make decisions accordingly, but be cautious of potential distortions. In practice mean and median together should also be viewed to highlight these insights with a measure such as median also being evaluated for skew where applicable in such circumstances..</li>
</ul>


      <footer>
        <p>© 2025 </p>
      </footer>
    </main>
  </body>
</html>
